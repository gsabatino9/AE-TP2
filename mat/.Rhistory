library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.7)
aux_test <- (-aux_train)
train <- por[aux_train, ]
test <- por[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
## Puedo fittear modelos y sacar las variables significativas
## para cada modelo.
## Acá voy a empezar sacando:
## G1, G2, absences, failures, course_type.
train$freetime <- as.numeric(train$freetime)
train$famrel <- as.numeric(train$famrel)
train$goout <- as.numeric(train$goout)
train$Dalc <- as.numeric(train$Dalc)
train$Walc <- as.numeric(train$Walc)
train$health <- as.numeric(train$health)
test$freetime <- as.numeric(test$freetime)
test$famrel <- as.numeric(test$famrel)
test$goout <- as.numeric(test$goout)
test$Dalc <- as.numeric(test$Dalc)
test$Walc <- as.numeric(test$Walc)
test$health <- as.numeric(test$health)
comparaciones
comparaciones
comparaciones[comparaciones$Modelo == "Random Forest", 2]
## Genero nuevas VAs con un árbol de decisión ----
mejor <- comparaciones[comparaciones$Modelo == "Random Forest", 2]
# absences
aux <- tree(G3 ~ absences, data=train)
plot(aux)
text(aux, pretty=0)
# age
aux <- tree(G3 ~ age, data=train)
plot(aux)
text(aux, pretty=0)
train2 <- copy(train)
train2 <- train
test2 <- test
train2$age_tree <- train$age < 18.8
test2$age_tree <- test$age < 18.8
# reason
aux <- tree(G3 ~ reason, data=train)
plot(aux)
text(aux, pretty=0)
train2$age_tree <- train[train$reason %in% c("course", "other"), ]
train2$age_tree <- train$age < 18.5
test2$age_tree <- test$age < 18.5
train2$reason <- train$reason %in% c("course", "other")
test2$reason <- test$reason %in% c("course", "other")
# Mjob
aux <- tree(G3 ~ Mjob, data=train)
plot(aux)
text(aux, pretty=0)
train2$Mjob <- train$Mjob %in% c("at_home", "other", "services")
test2$Mjob <- test$Mjob %in% c("at_home", "other", "services")
# Medu
aux <- tree(G3 ~ Medu, data=train)
plot(aux)
text(aux, pretty=0)
train$Medu
train2$Medu <- train$Medu %in% c(0,1,2,3)
test2$Medu <- test$Medu %in% c(0,1,2,3)
train2 <- train
test2 <- test
train2$age_tree <- train$age < 18.5
test2$age_tree <- test$age < 18.5
train2$reason_tree <- train$reason %in% c("course", "other")
test2$reason_tree <- test$reason %in% c("course", "other")
train2$Mjob_tree <- train$Mjob %in% c("at_home", "other", "services")
test2$Mjob_tree <- test$Mjob %in% c("at_home", "other", "services")
train2$Medu_tree <- train$Medu %in% c(0,1,2,3)
test2$Medu_tree <- test$Medu %in% c(0,1,2,3)
train2$Mjob_tree
# freetime
aux <- tree(G3 ~ freetime, data=train)
plot(aux)
text(aux, pretty=0)
train2$freetime_tree <- train$freetime < 4.5
test2$freetime_tree <- test$freetime < 4.5
# studytime
aux <- tree(G3 ~ studytime, data=train)
plot(aux)
text(aux, pretty=0)
train2$studytime_tree <- train$studytime < 1.5
test2$studytime_tree <- test$studytime < 1.5
# traveltime
aux <- tree(G3 ~ traveltime, data=train)
plot(aux)
text(aux, pretty=0)
train2$traveltime_tree <- train$traveltime < 1.5
test2$traveltime_tree <- test$traveltime < 1.5
# Walc
aux <- tree(G3 ~ Walc, data=train)
plot(aux)
text(aux, pretty=0)
train2$Walc_tree <- train$Walc < 2.5
test2$Walc_tree <- test$Walc < 2.5
# Dalc
aux <- tree(G3 ~ Dalc, data=train)
plot(aux)
text(aux, pretty=0)
train2$Dalc_tree <- train$Dalc < 1.5
test2$Dalc_tree <- test$Dalc < 1.5
# goout
aux <- tree(G3 ~ goout, data=train)
plot(aux)
text(aux, pretty=0)
train2$goout_tree <- train$goout < 4.5
test2$goout_tree <- test$goout < 4.5
# Fedu
aux <- tree(G3 ~ Fedu, data=train)
plot(aux)
text(aux, pretty=0)
train2$Fedu_tree <- train$Fedu %in% c(0,1,2)
test2$Fedu_tree <- test$Fedu %in% c(0,1,2)
# Fjob
aux <- tree(G3 ~ Fjob, data=train)
plot(aux)
text(aux, pretty=0)
train2$Fjob_tree <- train$Fjob %in% c("at_home", "other", "services")
test2$Fjob_tree <- test$Fjob %in% c("at_home", "other", "services")
# health
aux <- tree(G3 ~ health, data=train)
plot(aux)
text(aux, pretty=0)
train2$healt_tree <- train$healt < 2.5
test2$healt_tree <- test$healt < 2.5
## Fitteo mejor modelo sobre anterior ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train2, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test2)
comparaciones <- agregar_modelo("Random Forest 2", y.hat.rf)
comparaciones
varImpPlot(rf.fit)
setwd("~/Documents/AE/tp2/mat")
# MATEMÁTICA ----
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train <- mat[aux_train, ]
test <- mat[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
train$freetime <- as.numeric(train$freetime)
train$famrel <- as.numeric(train$famrel)
train$goout <- as.numeric(train$goout)
train$Dalc <- as.numeric(train$Dalc)
train$Walc <- as.numeric(train$Walc)
train$health <- as.numeric(train$health)
test$freetime <- as.numeric(test$freetime)
test$famrel <- as.numeric(test$famrel)
test$goout <- as.numeric(test$goout)
test$Dalc <- as.numeric(test$Dalc)
test$Walc <- as.numeric(test$Walc)
test$health <- as.numeric(test$health)
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 8.1731
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre, pred) {
ECM <- calcular_ecm(pred)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
comparaciones
## Árbol crecido ----
longtree.fit <- tree(G3 ~ ., train,
control=tree.control(nobs = nrow(train), mindev = 0))
y.hat.tree <- predict(longtree.fit, test)
comparaciones <- agregar_modelo("Árbol crecido", y.hat.tree)
comparaciones
## Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
y.hat.prunning <- predict(prunning.fit, test)
comparaciones <- agregar_modelo("Árbol podado", y.hat.prunning)
comparaciones
## Bagging ----
set.seed(9)
bag.fit <- randomForest(G3 ~ ., data=train, mtry=p, importance=TRUE)
y.hat.bag <- predict(bag.fit, test)
comparaciones <- agregar_modelo("Bagging", y.hat.bag)
comparaciones
varImpPlot(bag.fit)
## Boosting ----
boost.fit <- gbm(G3 ~ ., data=train,
distribution="gaussian", n.trees=5000)
summary(boost.fit)
y.hat.boost <- predict(boost.fit, test)
comparaciones <- agregar_modelo("Boosting", y.hat.boost)
comparaciones
## Random Forest ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
comparaciones <- agregar_modelo("Random Forest", y.hat.rf)
comparaciones
varImpPlot(rf.fit)
xtrain <- train[, -c(33)]
ytrain <- train$G3
xtest <- test[, -c(33)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
ECM_bart
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord] # Dalc como novedad
comparaciones
varImp(bag.fit)
importance <- varImp(bag.fit)
ord <- order(importance, decreasing=T)
importance$Overall
ord <- order(importance$Overall, decreasing=T)
importance$Overall[ord]
importance[importance$Overall[ord], ]
importance[1]
importance[, 1]
importance[, 0]
importance[, 2]
varImp(bag.fit, scale=F)
order(importance$Overall, decreasing=T)
importance[ord, ]
importance[ord]
importance[ord, ]
row.names(importance)
importance
importance$features <- row.names(importance)
importance[ord, ]
varImpPlot(bag.fit)
plot(importance)
plot(importance, top=20)
plot(varImp(bag.fit))
plot(varImp(bag.fit), top=20)
importance(bag.fit)
ls()
rm("importance")
importance
importance(bag.fit)
imps <- importance(bag.fit)
imps$IncNodePurity
imps[, 2]
imps[, 3]
imps[, 1]
ord <- order(importance[, 2], decreasing=T)
imps[1, 2]
imps[, 2]
imps <- importance(bag.fit, scale=F)
imps
imps <- importance(bag.fit, scale=T)
imps
imps <- importance(bag.fit)
order(importance[, 2], decreasing=T)
importance[, 2]
ord <- order(imps[, 2], decreasing=T)
ord
imps[ord, ]
imps[ord, ]
imps[ord, 2]
sum(imps[ord, 2])
importancias <- imps[ord, 2]
total <- sum(importancias)
(importancias / total) * 100
comparaciones
modelo <- lm(G3 ~ ., data=train)
importance(modelo)
varImp(modelo)
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train <- mat[aux_train, ]
test <- mat[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
## Puedo fittear modelos y sacar las variables significativas
## para cada modelo.
## Acá voy a empezar sacando:
## G1, G2, absences, failures, course_type.
train$freetime <- as.numeric(train$freetime)
train$famrel <- as.numeric(train$famrel)
train$goout <- as.numeric(train$goout)
train$Dalc <- as.numeric(train$Dalc)
train$Walc <- as.numeric(train$Walc)
train$health <- as.numeric(train$health)
test$freetime <- as.numeric(test$freetime)
test$famrel <- as.numeric(test$famrel)
test$goout <- as.numeric(test$goout)
test$Dalc <- as.numeric(test$Dalc)
test$Walc <- as.numeric(test$Walc)
test$health <- as.numeric(test$health)
## Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 27.86631
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo, "VAs"="-")
agregar_modelo <- function(nombre, pred, variables) {
ECM <- calcular_ecm(pred)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM, "VAs"=variables))
)
}
## Regresión con todos los predictores ----
lm.fit <- lm(G3 ~ ., data=train)
summary(lm.fit)
"
Cosas importantes a destacar:
- p-valor de la regresión: < 2.2e-16 (prácticamente 0). Regresión es significativa,
entonces tiene sentido plantear un modelo lineal con estos predictores.
- R^2 ajustado: 0.8426 -> Explica muy bien la varianza del modelo.
- No hay prácticamente predictores significativos por su cuenta,
a excepción de G2. Pero, puede que alguno de ellos se deba a un error
(aunque es muy difícil puesto que el p-valor de la regresión es casi 0).
"
y.hat <- predict(lm.fit, test)
comparaciones <- agregar_modelo("Reg. lineal completo", y.hat, "Todas")
## Saco los que tienen p-valor > 0.05 (aprox) ----
sacar <- c("school", "sex", "address", "famsize", "Pstatus",
"Medu", "Mjob", "Fjob", "reason", "guardian",
"traveltime", "freetime", "schoolsup", "famsup",
"paid", "activities", "nursery", "internet",
"famrel", "Dalc", "Walc", "health", "absences")
train2 <- train[, !(names(train) %in% sacar)]
test2 <- test[, !(names(test) %in% sacar)]
nuevo_modelo <- lm(G3 ~ ., data=train2)
summary(nuevo_modelo)
comparaciones <- agregar_modelo(
"Quitando VAs rl simple", predict(nuevo_modelo, test),
"age, Fedu, studytime, failures, higher, romantic, goout, G1, G2")
## Genero un árbol de decisión por c/u de las variables quitadas ----
# Medu
aux <- tree(G3 ~ Medu, data=train)
plot(aux)
text(aux, pretty=0) # 1,2,3
train2$Medu <- train$Medu %in% c(1,2,3)
test2$Medu <- test$Medu %in% c(1,2,3)
# Mjob
aux <- tree(G3 ~ Mjob, data=train)
plot(aux)
text(aux, pretty=0)
train2$Mjob <- train$Mjob %in% c("at_home", "other")
test2$Mjob <- test$Mjob %in% c("at_home", "other")
# reason
aux <- tree(G3 ~ reason, data=train)
plot(aux)
text(aux, pretty=0)
train2$reason <- train$reason %in% c("course", "home")
test2$reason <- test$reason %in% c("course", "home")
# freetime
aux <- tree(G3 ~ freetime, data=train)
plot(aux)
text(aux, pretty=0)
train2$freetime <- train$freetime < 2.5
test2$freetime <- test$freetime < 2.5
# Dalc
aux <- tree(G3 ~ Dalc, data=train)
plot(aux)
text(aux, pretty=0)
train2$Dalc <- train$Dalc < 1.5
test2$Dalc <- test$Dalc < 1.5
# health
aux <- tree(G3 ~ health, data=train)
plot(aux)
text(aux, pretty=0)
train2$health <- train$health < 1.5
test2$health <- test$health < 1.5
# absences
aux <- tree(G3 ~ absences, data=train)
plot(aux)
text(aux, pretty=0)
train2$absences <- train$absences < 0.5
test2$absences <- test$absences < 0.5
## Fitteo con las columnas nuevas ----
nuevo_modelo2 <- lm(G3 ~ ., data=train2)
summary(nuevo_modelo2)
p <- ncol(train2)-1
comparaciones <- agregar_modelo(
"Cols generadas de árboles", predict(nuevo_modelo2, test2), "...")
# Mejora muchísimo.
## LASSO ----
x.train <- model.matrix(G3 ~ ., train2)[, -1]
y.train <- train2$G3
x.test <- model.matrix(G3 ~ ., test2)[, -1]
y.test <- test2$G3
grid <- 10^seq(10, -2, length=100)
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
best_lambda <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x.test)
comparaciones <- agregar_modelo("Lasso", lasso.pred, "...")
comparaciones
# Mejora muchísimo.
## Selección de modelos ----
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train2[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train2[folds == j, ], id=i)
cv.errors[j, i] <- mean((train2$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd)
coef(regfit.fwd, coef_fwd)
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ ., data=train2, nvmax=p, method="forward")
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd)
coef(regfit.fwd, coef_fwd)
fwd.fit <- lm(G3 ~ G1+G2+absences, data=train2)
y.hat <- predict(fwd.fit, test2)
comparaciones <- agregar_modelo("FWD selection 2",
y.hat, "G1+G2+absences")
comparaciones
### Mixed-Selection por AIC
set.seed(9)
step.model <- stepAIC(nuevo_modelo2, direction="both", trace=FALSE)
summary(step.model)
y.hat <- predict(step.model, test2)
comparaciones <- agregar_modelo("MIXED selection AIC", y.hat,
"age, studytime, G1, G2, Dalc, absences")
comparaciones
varImp(step.model)
source("~/Documents/AE/tp2/mat/modelo_lineal.R", echo=TRUE)
