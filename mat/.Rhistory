alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
alpha.fn(Portfolio, 1:100)
# una mini prueba del paso (2)
set.seed(7)
## Selecciono 100 observaciones (con repo) que van desde 1 a 100
alpha.fn(Portfolio, sample(100, 100, replace=TRUE))
# 2. con 1000 muestras bootstrapeadas
boot(Portfolio, alpha.fn, R=1000)
## Estimando Accuracy de una regresión lineal
boot.fn <- function(data, index)
## Estimando Accuracy de una regresión lineal
boot.fn <- function(data, index) {
coef(lm(mpg ~ horsepower, data=data, subset=index))
}
boot.fn(Auto, 1:392)
## Estimando Accuracy de una regresión lineal
boot.fn <- function(data, index) {
coef(lm(mpg ~ horsepower, data=data, subset=index))
}
boot.fn(Auto, 1:392)
# puedo hacer lo mismo que hice antes de usar muestras bootstrapeadas
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=TRUE))
boot.fn(Auto, sample(392, 392, replace=TRUE))
# puedo hacer lo mismo que hice antes de usar muestras bootstrapeadas
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=TRUE))
boot.fn(Auto, sample(392, 392, replace=TRUE))
# 2.
boot(Auto, boot.fn, R=1000)
## Me da prácticamente los coeficientes que me da summary(lm(...)):
summary(lm(mpg ~ horsepower, data=Auto))$coef
setwd("~/Documents/AE/tp2/mat")
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train <- mat[aux_train, ]
test <- mat[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
## Puedo fittear modelos y sacar las variables significativas
## para cada modelo.
## Acá voy a empezar sacando:
## G1, G2, absences, failures, course_type.
train$freetime <- as.numeric(train$freetime)
train$famrel <- as.numeric(train$famrel)
train$goout <- as.numeric(train$goout)
train$Dalc <- as.numeric(train$Dalc)
train$Walc <- as.numeric(train$Walc)
train$health <- as.numeric(train$health)
test$freetime <- as.numeric(test$freetime)
test$famrel <- as.numeric(test$famrel)
test$goout <- as.numeric(test$goout)
test$Dalc <- as.numeric(test$Dalc)
test$Walc <- as.numeric(test$Walc)
test$health <- as.numeric(test$health)
train$G3 <- ifelse(train$G3 == 0, 1, train$G3)
train$G2 <- ifelse(train$G2 == 0, 1, train$G2)
train$G1 <- ifelse(train$G1 == 0, 1, train$G1)
test$G3 <- ifelse(test$G3 == 0, 1, test$G3)
test$G2 <- ifelse(test$G2 == 0, 1, test$G2)
test$G1 <- ifelse(test$G1 == 0, 1, test$G1)
## Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 27.86631
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre, pred) {
ECM <- calcular_ecm(pred)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
## Regresión con todos los predictores ----
lm.fit <- lm(G3 ~ ., data=train)
summary(lm.fit)
y.hat <- predict(lm.fit, test)
comparaciones <- agregar_modelo("Reg. lineal completo", y.hat)
comparaciones
## Selección de modelos ----
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ ., data=train, nvmax=p, method="forward")
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd)
coef(regfit.fwd, coef_fwd)
fwd.fit <- lm(G3 ~ nursery+famrel+absences+G1+G2, data=train)
y.hat <- predict(fwd.fit, test)
comparaciones <- agregar_modelo("FWD selection",
y.hat)
comparaciones
### Mixed-Selection por AIC
set.seed(9)
step.model <- stepAIC(lm.fit, direction="both", trace=FALSE)
summary(step.model)
y.hat <- predict(step.model, test)
comparaciones <- agregar_modelo("MIXED selection AIC", y.hat)
comparaciones
school <- lm(G3 ~ school, data=train)
summary(school) # 0.6871
sex <- lm(G3 ~ sex, data=train)
summary(sex) # 0.0905
address <- lm(G3 ~ address, data=train)
summary(address) # 0.1405
famsize <- lm(G3 ~ famsize, data=train)
summary(famsize) # 0.1385
Pstatus <- lm(G3 ~ Pstatus, data=train)
summary(Pstatus) # 0.2711
Medu <- lm(G3 ~ Medu, data=train)
summary(Medu) # 0.09354
Fedu <- lm(G3 ~ Fedu, data=train)
summary(Fedu) # 0.06633
Mjob <- lm(G3 ~ Mjob, data=train)
summary(Mjob) # 0.1387
Fjob <- lm(G3 ~ Fjob, data=train)
summary(Fjob) # 0.609
reason <- lm(G3 ~ reason, data=train)
summary(reason) # 0.3631
# 11. guardian:
guardian <- lm(G3 ~ guardian, data=train)
summary(guardian) # 0.7339
absences <- lm(G3 ~ absences, data=train)
summary(absences) # 0.4432
## Saco los que tienen p-valor > 0.05 (aprox) ----
sacar <- c("school", "sex", "address", "famsize", "Pstatus",
"Medu", "Mjob", "Fjob", "reason", "guardian",
"traveltime", "freetime", "schoolsup", "famsup",
"paid", "activities", "nursery", "internet",
"famrel", "Dalc", "Walc", "health", "absences")
train2 <- train[, !(names(train) %in% sacar)]
test2 <- test[, !(names(test) %in% sacar)]
nuevo_modelo <- lm(G3 ~ ., data=train2)
comparaciones <- agregar_modelo(
"Quitando VAs rl simple", predict(nuevo_modelo, test))
comparaciones
## Genero un árbol de decisión por c/u de las variables quitadas ----
# Medu
aux <- tree(G3 ~ Medu, data=train)
plot(aux)
text(aux, pretty=0) # 1,2,3
train2$Medu <- train$Medu %in% c(1,2,3)
test2$Medu <- test$Medu %in% c(1,2,3)
# Mjob
aux <- tree(G3 ~ Mjob, data=train)
plot(aux)
text(aux, pretty=0)
train2$Mjob <- train$Mjob %in% c("at_home", "other")
test2$Mjob <- test$Mjob %in% c("at_home", "other")
# reason
aux <- tree(G3 ~ reason, data=train)
plot(aux)
## Fitteo con las columnas nuevas ----
nuevo_modelo2 <- lm(G3 ~ ., data=train2)
p <- ncol(train2)-1
comparaciones <- agregar_modelo(
"Cols generadas de árboles", predict(nuevo_modelo2, test2))
comparaciones
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ ., data=train2, nvmax=p, method="forward")
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train2[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train2[folds == j, ], id=i)
cv.errors[j, i] <- mean((train2$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd)
coef(regfit.fwd, coef_fwd)
fwd.fit <- lm(G3 ~ G1+G2, data=train2)
y.hat <- predict(fwd.fit, test2)
comparaciones <- agregar_modelo("FWD selection 2",
y.hat)
comparaciones
set.seed(9)
step.model <- stepAIC(nuevo_modelo2, direction="both", trace=FALSE)
summary(step.model)
y.hat <- predict(step.model, test2)
comparaciones <- agregar_modelo("MIXED selection AIC", y.hat)
comparaciones
calcular_ecm2 <- function(modelo, a_testear) {
y.hat <- predict(modelo, a_testear)
return(mean((y.hat - a_testear$G3)^2))
}
calcular_ecm2(fwd.fit, test2)
comparaciones
calcular_ecm <- function(modelo, a_testear) {
y.hat <- predict(modelo, a_testear)
return(mean((y.hat - a_testear$G3)^2))
}
agregar_modelo <- function(nombre, modelo, a_testear) {
ECM <- calcular_ecm(modelo, a_testear)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
x.train <- model.matrix(G3 ~ ., train2)[, -1]
y.train <- train2$G3
x.test <- model.matrix(G3 ~ ., test2)[, -1]
y.test <- test2$G3
grid <- 10^seq(10, -2, length=100)
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
best_lambda <- cv.out$lambda.min
coef(lasso.mod, s=best_lambda)
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x.test)
lasso.pred
ecm_predicciones <- function(predicciones, a_testear) {
return(mean((predicciones - a_testear$G3)^2))
}
ecm_predicciones(lasso.pred, test2)
lm2 <- lm(G3 ~ age+Fedu+studytime+failures+romantic+G1+G2+reason+Dalc+absences,
data=train2)
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train <- mat[aux_train, ]
test <- mat[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
## Puedo fittear modelos y sacar las variables significativas
## para cada modelo.
## Acá voy a empezar sacando:
## G1, G2, absences, failures, course_type.
train$freetime <- as.numeric(train$freetime)
train$famrel <- as.numeric(train$famrel)
train$goout <- as.numeric(train$goout)
train$Dalc <- as.numeric(train$Dalc)
train$Walc <- as.numeric(train$Walc)
train$health <- as.numeric(train$health)
test$freetime <- as.numeric(test$freetime)
test$famrel <- as.numeric(test$famrel)
test$goout <- as.numeric(test$goout)
test$Dalc <- as.numeric(test$Dalc)
test$Walc <- as.numeric(test$Walc)
test$health <- as.numeric(test$health)
## Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(modelo, a_testear) {
y.hat <- predict(modelo, a_testear)
return(mean((y.hat - a_testear$G3)^2))
}
ecm_predicciones <- function(predicciones, a_testear) {
return(mean((predicciones - a_testear$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 27.86631
ECM_modelo_nulo <- ecm_predicciones(y.hat) # 27.86631
ECM_modelo_nulo <- ecm_predicciones(y.hat, test) # 27.86631
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre, modelo, a_testear) {
ECM <- calcular_ecm(modelo, a_testear)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
## Regresión con todos los predictores ----
lm.fit <- lm(G3 ~ ., data=train)
summary(lm.fit)
View(train[train$G1 & train$G2 & train$G3, ])
# Forma 2: Los remuevo directamente:
train <- train[train$G1 & train$G2 & train$G3, ]
test <- test[test$G1 & test$G2 & test$G3, ]
## Regresión con todos los predictores ----
lm.fit <- lm(G3 ~ ., data=train)
summary(lm.fit)
comparaciones <- agregar_modelo("Reg. lineal completo", lm.fit, test)
comparaciones
nrow(test)
nrow(train)
nrow(train)+nrow(test)
nrow(mat)
## Selección de modelos ----
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ ., data=train, nvmax=p, method="forward")
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
n <- nrow(train)
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd)
coef(regfit.fwd, coef_fwd)
#fwd.fit <- lm(G3 ~ nursery+famrel+absences+G1+G2, data=train)
fwd.fit <- lm(G3 ~ Mjob+nursery+absences+G1+G2+famrel+goout+health
, data=train)
comparaciones <- agregar_modelo("FWD selection",
fwd.fit, test)
comparaciones
school <- lm(G3 ~ school, data=train)
summary(school) # 0.6871
sex <- lm(G3 ~ sex, data=train)
summary(sex) # 0.0905
summary(lm.fit)
address <- lm(G3 ~ address, data=train)
summary(address) # 0.1405
famsize <- lm(G3 ~ famsize, data=train)
summary(famsize) # 0.1385
Pstatus <- lm(G3 ~ Pstatus, data=train)
summary(Pstatus) # 0.2711
Medu <- lm(G3 ~ Medu, data=train)
summary(Medu) # 0.09354
## LASSO ----
train2 <- train
test2 <- test
x.train <- model.matrix(G3 ~ ., train2)[, -1]
y.train <- train2$G3
x.test <- model.matrix(G3 ~ ., test2)[, -1]
y.test <- test2$G3
grid <- 10^seq(10, -2, length=100)
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
best_lambda <- cv.out$lambda.min
coef(lasso.mod, s=best_lambda)
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x.test)
ecm_predicciones(lasso.pred, test2)
train$absences <- sqrt(train$absences)
test$absences <- sqrt(test$absences)
train2 <- train
test2 <- test
x.train <- model.matrix(G3 ~ ., train2)[, -1]
y.train <- train2$G3
x.test <- model.matrix(G3 ~ ., test2)[, -1]
y.test <- test2$G3
grid <- 10^seq(10, -2, length=100)
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
best_lambda <- cv.out$lambda.min
coef(lasso.mod, s=best_lambda)
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x.test)
ecm_predicciones(lasso.pred, test2)
confint(lm.fit)
confint(lasso.mod)
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ ., data=train, nvmax=p, method="forward")
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd)
coef(regfit.fwd, coef_fwd)
#fwd.fit <- lm(G3 ~ nursery+famrel+absences+G1+G2, data=train)
fwd.fit <- lm(G3 ~ Mjob+nursery+absences+G1+G2+famrel+goout+health
, data=train)
comparaciones <- agregar_modelo("FWD selection",
fwd.fit, test)
comparaciones
confint(fwd.fit)
a <- confint(fwd.fit)
a[1]
a[1,]
a[,1]
a <- confint(fwd.fit)
l <- nrow(a)
a[1,2]
confs <- data.frame(Beta=k, "+-"=a[1,2]-k)
confs
confs <- data.frame(Beta=k, confianza=a[1,2]-k)
confs
a[1,2]
a[1,1]
k
a[1,2]+a[1,1]
(a[1,2]+a[1,1])/2
k <- (a[1,2]+a[1,1])/2
confs <- data.frame(Beta=k, confianza=a[1,2]-k)
confs
confs <- matrix(1:(2*l), nrow=l, ncol=2)
confs
confs <- matrix(NA, nrow=l, ncol=2)
confs
confs[1,] <- c(1,1)
confs
confs <- matrix(NA, nrow=l, ncol=2)
for (i in 1:l) {
beta <- (a[i,2]+a[i,1])/2
intervalo <- a[i,2]-beta
confs[i,] <- c(beta,intervalo)
}
confs
names(fwd.fit)
fwd.fit$coefficients
names(fwd.fit$coefficients)
confs[,3] <- names(fwd.fit$coefficients)
confs[,2]
row.names(confs) <- names(fwd.fit$coefficients)
confs
summary(fwd.fit)
names(confs) <- c("Beta", "+-")
confs
col.names(confs) <- c("Beta", "+-")
