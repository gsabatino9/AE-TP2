# puedo hacer lo mismo que hice antes de usar muestras bootstrapeadas
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=TRUE))
boot.fn(Auto, sample(392, 392, replace=TRUE))
# puedo hacer lo mismo que hice antes de usar muestras bootstrapeadas
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=TRUE))
boot.fn(Auto, sample(392, 392, replace=TRUE))
# 2.
boot(Auto, boot.fn, R=1000)
## Me da prácticamente los coeficientes que me da summary(lm(...)):
summary(lm(mpg ~ horsepower, data=Auto))$coef
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
setwd("~/Documents/AE/tp2")
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
load_libraries()
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
por["course_type"] = rep("P", nrow(por))
mat["course_type"] = rep("M", nrow(mat))
df <- rbind(por, mat)
df["course_type"] = lapply(df["course_type"], factor)
set.seed(9)
aux_train <- sample(1:nrow(df), nrow(df)*0.7)
aux_test <- (-aux_train)
train <- df[aux_train, ]
test <- df[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
str(train)
# 1. School:
school <- lm(G3 ~ school, data=train)
summary(school)
# 2. Sex:
school <- lm(G3 ~ "school", data=train)
# 2. Sex:
sex <- lm(G3 ~ sex, data=train)
summary(sex) # 0.001018
# 3. address:
address <- lm(G3 ~ address, data=train)
summary(address)
# 4. famsize:
famsize <- lm(G3 ~ famsize, data=train)
summary(famsize) # 0.0002915
# 5. Pstatus:
Pstatus <- lm(G3 ~ Pstatus, data=train)
summary(Pstatus) # 0.2398
# 6. Mstatus:
Mstatus <- lm(G3 ~ Mstatus, data=train)
# 6. Medu:
Medu <- lm(G3 ~ Medu, data=train)
summary(Medu) # 0.3585
# 7. Fedu:
Fedu <- lm(G3 ~ Fedu, data=train)
summary(Fedu) # 1.4e-07
# 8. Mjob:
Mjob <- lm(G3 ~ Mjob, data=train)
summary(Mjob) # 9.549e-05
# 9. Fjob:
Fjob <- lm(G3 ~ Fjob, data=train)
summary(Fjob) # 0.0001244
# 10. reason:
reason <- lm(G3 ~ reason, data=train)
summary(reason) # 0.02104
# 11. guardian:
guardian <- lm(G3 ~ guardian, data=train)
summary(guardian) # 0.01054
# 12. traveltime:
traveltime <- lm(G3 ~ traveltime, data=train)
summary(traveltime) # 0.01098
# 13. freetime:
freetime <- lm(G3 ~ freetime, data=train)
summary(freetime) # 0.007925
train$freetime
as.numeric(train$freetime)
train$freetime <- as.numeric(train$freetime)
# 13. freetime:
freetime <- lm(G3 ~ freetime, data=train)
summary(freetime) # 0.008838
# 14. studytime:
studytime <- lm(G3 ~ studytime, data=train)
summary(studytime) # 0.06549
# 15. schoolsup:
schoolsup <- lm(G3 ~ schoolsup, data=train)
summary(schoolsup) # 0.00011
# 16. famsup:
famsup <- lm(G3 ~ famsup, data=train)
summary(famsup) # 0.01827
# 17. paid:
paid <- lm(G3 ~ paid, data=train)
summary(paid) # 0.5534
# 18. activities:
activities <- lm(G3 ~ activities, data=train)
summary(activities) # 0.1035
# 19. nursery:
nursery <- lm(G3 ~ nursery, data=train)
summary(nursery) # 0.7379
# 20. higher:
higher <- lm(G3 ~ higher, data=train)
summary(higher) # 0.1673
# 21. internet:
internet <- lm(G3 ~ internet, data=train)
summary(internet) # 1.191e
# 22. romantic:
romantic <- lm(G3 ~ romantic, data=train)
summary(romantic) # 0.002842
train$famrel <- as.numeric(train$famrel)
# 23. famrel:
famrel <- lm(G3 ~ famrel, data=train)
summary(famrel) # 0.01542
train$goout <- as.numeric(train$goout)
train$goout <- as.numeric(train$goout)
train$Dalc <- as.numeric(train$Dalc)
train$Walc <- as.numeric(train$Walc)
# 24. goout:
goout <- lm(G3 ~ goout, data=train)
summary(goout) # 0.6447
# 25. Dalc:
Dalc <- lm(G3 ~ Dalc, data=train)
summary(Dalc) # 0.01057
# 26. Walc:
Walc <- lm(G3 ~ Walc, data=train)
summary(Walc) # 0.0007654
train$health <- as.numeric(train$health)
# 27. health:
health <- lm(G3 ~ health, data=train)
summary(health) # 0.00205
summary(higher) # 1.191e
test$freetime <- as.numeric(test$freetime)
test$famrel <- as.numeric(test$famrel)
test$goout <- as.numeric(test$goout)
test$Dalc <- as.numeric(test$Dalc)
test$Walc <- as.numeric(test$Walc)
test$health <- as.numeric(test$health)
# Empiezo quitando las que dan alto ----
new_model <- lm(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, data=train)
### Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 14.92127
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre_modelo, resultado) {
return(
rbind(comparaciones, data.frame(Modelo=nombre_modelo, "ECM test"=resultado))
)
}
### Regresión lineal con todos los predictores ----
lm.fit <- lm(G3 ~ ., data=train)
summary(lm.fit)
y.hat <- predict(lm.fit, test)
ECM_reg_lineal1 <- calcular_ecm(y.hat)
comparaciones <- agregar_modelo("Reg. lineal 1", ECM_reg_lineal1)
calcular_ecm(predict(new_model, test))
comparaciones
### Selección de modelos ----
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, data=train, nvmax=p, method="forward")
summary(regfit.fwd)
# Me quedo con el mejor a partir de cv con K=10:
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
coef_fwd <- which.min(mean.cv.errors.fwd) # 2
coef(regfit.fwd, coef_fwd) # se queda solo con G1 y G2.
fwd.fit <- lm(G3 ~ G1 + G2, data=train)
y.hat <- predict(fwd.fit, test)
ECM_rl_fwd <- calcular_ecm(y.hat)
comparaciones <- agregar_modelo("FWD selection", ECM_rl_fwd)
comparaciones
### Mixed-Selection por AIC
set.seed(9)
step.model <- stepAIC(new_model, direction="both", trace=FALSE)
summary(step.model)
y.hat <- predict(step.model, test)
ECM_mix_aic <- calcular_ecm(y.hat)
comparaciones <- agregar_modelo("MIXED selection AIC", ECM_mix_aic)
comparaciones
### RIDGE ----
x.train <- model.matrix(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, train)[, -1]
y.train <- train$G3
x.test <- model.matrix(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, test)[, -1]
y.test <- test$G3
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x.train, y.train, alpha=0, lambda=grid, thresh=1e-12)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out)
best_lambda <- cv.out$lambda.min
best_lambda
ridge.pred <- predict(ridge.mod, s=best_lambda, newx=x.test)
ECM_ridge <- calcular_ecm(ridge.pred)
comparaciones <- agregar_modelo("Ridge", ECM_ridge)
comparaciones
### LASSO ----
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
best_lambda <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x.test)
ECM_lasso <-calcular_ecm(lasso.pred)
comparaciones <- agregar_modelo("Lasso", ECM_lasso)
comparaciones
lasso.mod$lamda == best_lambda
lasso.mod$lambda == best_lambda
sum(lasso.mod$lambda == best_lambda)
coef(cv.out)
coef(ridge.mod)
coef(cv.out)
comparaciones
### Árbol de decisión ----
longtree.fit <- tree(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, train,
control=tree.control(nobs = nrow(train), mindev = 0))
y.hat.tree <- predict(longtree.fit, test)
ECM_arbol_total <- calcular_ecm(y.hat.tree)
comparaciones <- agregar_modelo("Árbol crecido", ECM_arbol_total)
comparaciones
#### Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
### Árbol de decisión sin G1 y G2 ----
longtree.fit <- tree(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
-G1-G2, train,
control=tree.control(nobs = nrow(train), mindev = 0))
#### Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
## Uso esa info para generar nuevas columnas:
train$failures
train[train$failures == 0]
train[train$failures == 0, ]
train$failures == 0
## Uso esa info para generar nuevas columnas:
train$failures_tree <- train$failures == 0
train$absences_tree <- train$absences == 0
train$Medu_tree <- train$Medu < 4
train$Medu
as.numeric(train$Medu)
as.numeric(train$Medu)-1
train$Medu_tree <- as.numeric(train$Medu)-1 < 4
train$health_tree <- train$health < 5
train$age_tree <- train$age < 17
longtree.fit2 <- tree(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
-G1-G2, train,
control=tree.control(nobs = nrow(train), mindev = 0))
prunning.fit2 <- prune.tree(longtree.fit2, best=5)
plot(prunning.fit2)
text(prunning.fit2, pretty=0)
new_model2 <- lm.fit(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, data=train)
new_model2 <- lm.fit(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, data=train)
train
new_model2 <- lm.fit(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, data=train)
new_model2 <- lm(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, data=train)
test$failures_tree <- test$failures == 0
test$absences_tree <- test$absences == 0
test$Medu_tree <- as.numeric(test$Medu)-1 < 4
test$health_tree <- test$health < 5
test$age_tree <- test$age < 17
calcular_ecm(predict(new_model2, test))
summary(new_model2)
train$Medu_tree
str(train)
as.factor(train$Medu_tree)
new_model2 <- lm(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel, data=train)
summary(new_model2)
is.na(train$Medu_tree)
sum(is.na(train$Medu_tree))
summary(new_model2)
as.numeric(test$Medu)-1 < 4
test$health < 5
as.numeric(train$Medu)-1 < 4
length(train$Medu_tree)
n
new_model2 <- lm(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel-Medu_tree, data=train)
summary(new_model2)
calcular_ecm(predict(new_model2, test))
comparaciones
summary(new_model2)
# 1. G1 y G2:
g1_g2_tree <- tree(G3 ~ G1+G2, data=train)
plot(g1_g2_tree)
text(g1_g2_tree, pretty=0)
# 2. Medu:
medu_tree <- tree(G3 ~ Medu, data=train)
plot(medu_tree)
text(medu_tree, pretty=0)
train$Medu
# 3. Fedu:
fedu_tree <- tree(G3 ~ Fedu, data=train)
plot(fedu_tree)
text(fedu_tree, pretty=0) # 1,2,3
# 4. Mjob:
Mjob_tree <- tree(G3 ~ Mjob, data=train)
plot(Mjob_tree)
text(Mjob_tree, pretty=0) # 0,1,2,3
train$Mjob
# 5. Fjob:
Fjob_tree <- tree(G3 ~ Fjob, data=train)
plot(Fjob_tree)
text(Fjob_tree, pretty=0) # at_home, other
summary(new_model2)
# 6. reason:
reason_tree <- tree(G3 ~ reason, data=train)
plot(reason_tree)
text(reason_tree, pretty=0)
# 7. guardian:
guardian_tree <- tree(G3 ~ guardian, data=train)
plot(guardian_tree)
text(guardian_tree, pretty=0) # course, home, other
# 8. traveltime, studytime, freetime:
time_tree <- tree(G3 ~ traveltime + studytime + freetime, data=train)
plot(time)
plot(time_tree)
text(time_tree, pretty=0)
summary(new_model2)
train$goout
# 9. goout:
goout_tree <- tree(G3 ~ goout, data=train)
plot(goout_tree)
text(goout_tree, pretty=0) # studytime < 2.5, traveltime < 1.5
# 10. Dalc, Walc:
alc <- tree(G2 ~ Dalc+Walc, data=train)
plot(alc)
text(alc, pretty=0)
# 10. Dalc, Walc:
alc <- tree(G3 ~ Dalc+Walc, data=train)
plot(alc)
text(alc, pretty=0) # Walc < 3.5, Dalc < 3.5
# 11. health:
health_tree <- tree(G3 ~ health, data=train)
plot(health_tree)
text(health_tree, pretty=0)
summary(health_tree)
# 11. health:
health_tree <- tree(G3 ~ health, data=train)
summary(health_tree)
plot(health_tree)
text(health_tree, pretty=0)
# 10. Dalc, Walc:
alc <- tree(G3 ~ Dalc+Walc+health, data=train)
plot(alc)
text(alc, pretty=0) # Dalc < 1.5
# 11. absences y G1:
rend <- tree(G3 ~ absences+G1, data=train)
plot(red)
plot(rend)
text(rend, pretty=0)
plot(time_tree)
text(time_tree, pretty=0) # studytime < 2.5, traveltime < 1.5
train$G2_tree <- train$G2 < 11.5
train$Medu == 1
train$Medu_tree <- train$Medu == 1 | train$Medu == 2 | train$Medu == 3
train$Fedu < 4
train$Fedu_tree <- train$Fedu == 1 | train$Fedu == 2
train$Fedu_tree <- train$Fedu == 1 | train$Fedu == 2 | train$Fedu == 3 | train$Fedu == 0
train
train$Fedu_tree
train$reason_tree <- train$reason == "other"
train$reason == "other"
train$Mjob %in% c("at_home", "other")
train$Mjob_tree <- train$Mjob %in% c("at_home", "other")
train$Fjob_tree <- train$Fjob %in% c("at_home", "other", "services")
train$studytime_tree <- train$studytime < 2.5
train$traveltime_tree <- train$traveltime < 1.5
train$goout_tree <- train$goout < 4.5
train$Dalc_tree <- train$Dalc < 1.5
train$G1_tree <- train$G1 < 10.5
test$G2_tree <- test$G2 < 11.5
test$Medu_tree <- test$Medu == 1 | test$Medu == 2 | test$Medu == 3
test$Fedu_tree <- test$Fedu == 1 | test$Fedu == 2 | test$Fedu == 3 | test$Fedu == 0
test$reason_tree <- test$reason == "other"
test$Mjob_tree <- test$Mjob %in% c("at_home", "other")
test$Fjob_tree <- test$Fjob %in% c("at_home", "other", "services")
test$studytime_tree <- test$studytime < 2.5
test$traveltime_tree <- test$traveltime < 1.5
test$goout_tree <- test$goout < 4.5
test$Dalc_tree <- test$Dalc < 1.5
test$G1_tree <- test$G1 < 10.5
# Fitteo modelos ----
regfit.fwd <- regsubsets(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, data=train, nvmax=p, method="forward")
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
mean.cv.errors.fwd
coef_fwd <- which.min(mean.cv.errors.fwd) # 2
coef(regfit.fwd, coef_fwd) # se queda solo con G1 y G2.
fwd.fit <- lm(G3 ~ G1 + G2, data=train)
y.hat <- predict(fwd.fit, test)
ECM_rl_fwd <- calcular_ecm(y.hat)
ECM_rl_fwd
# Fitteo modelos ----
## ML
arbol_modelo <- lm(G3 ~ .
-sex- famsize- Pstatus- freetime- famsup-
paid- activities- nursery- famrel
, data=train)
calcular_ecm(predict(arbol_modelo, test))
summary(arbol_modelo)
calcular_ecm(predict(arbol_modelo, test))
