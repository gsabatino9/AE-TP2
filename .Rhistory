# R^2:
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$rss, xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted R^2", type = "l")
max_adjr2 <- which.max(reg.summary$adjr2)
points(max_adjr2, reg.summary$adjr2[max_adjr2], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
min_cp <- which.min(reg.summary$cp)
points(min_cp, reg.summary$cp[min_cp], col = "red", cex = 2,
pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
min_bic <- which.min(reg.summary$bic)
points(min_bic, reg.summary$bic[min_bic], col = "red", cex = 2,
pch = 20)
## ----
## Grafico de regsubsets:
plot(regfit.full, scale = "r2")
## ----
## Grafico de regsubsets:
plot(regfit.full, scale = "r2")
## ----
## Grafico de regsubsets:
plot(regfit.full, scale = "r2")
## ----
## Grafico de regsubsets:
plot(regfit.full, scale = "r2")
## ----
## Grafico de regsubsets:
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
coef(regfit.full, 6)
#### Forward and Backward selection
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19, method = "backward")
summary(regfit.bwd)
#### Eligiendo entre modelos a partir de:
# 1. Validation-set:
## -----
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace=TRUE)
test <- (!train)
regfit.best <- regsubsets(Salary ~ ., data=Hitters[train,], nvmax=19)
test.mat <- model.matrix(Salary ~ ., data=Hitters[test,])
val.errors <- rep(NA, 19)
i <- 1
coef_i <- coef(regfit.best, id=i)
test.mat[, names(coef_i)] %*% coef_i
test.mat[, names(coef_i)]
val.errors <- rep(NA, 19)
test.mat[, names(coef_i)]
val.errors <- rep(NA, 19)
for (i in 1:19) {
coef_i <- coef(regfit.best, id=i)
# multiplico los datos que son de test en mi matrix,
# usando solo la columna de los coeficientes que acabo
# de calcular:
pred <- test.mat[, names(coef_i)] %*% coef_i
val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
which.min(val.errors) # mejor es 7
coef(regfit.best, 7)
# fabrico función para usar tipo predict():
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
# Como ya vi que el mejor es 7 => ahora calculo
# best-subset selection en el **data set entero** y elijo
# el que tiene 7.
regfit.best <- regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(regfit.best, 7)
## 2 CV: ----
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, 19,
dimnames = list(NULL, paste(1:19)))
View(cv.errors)
View(cv.errors)
best.fit <- regsubsets(Salary ~ ., data=Hitters[folds != j, ], nvmax=19)
ncol(Hitters)-1
p <- ncol(Hitters)-1
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ ., data=Hitters[folds != j, ], nvmax=p)
# loopeo p veces para ver cómo es cada uno de los modelos con
# i variables (i entre 1 y p)
for (i in 1:p) {
# uso la predict() que fabriqué arriba
pred <- predict(best.fit, Hitters[folds == j], id=i)
cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)
}
}
# media de cada columna:
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ ., data=Hitters[folds != j, ], nvmax=p)
# loopeo p veces para ver cómo es cada uno de los modelos con
# i variables (i entre 1 y p)
for (i in 1:p) {
# uso la predict() que fabriqué arriba
pred <- predict(best.fit, Hitters[folds == j], id=i)
cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)
}
}
j <- 4
i <- 5
best.fit <- regsubsets(Salary ~ ., data=Hitters[folds != j, ], nvmax=p)
predict(best.fit, Hitters[folds == j], id=i)
best.fit
regsubsets(Salary ~ ., data=Hitters[folds != j, ], nvmax=p)
predict(best.fit, Hitters[folds == j], id=i)
predict(best.fit, Hitters[folds == j, ], id=i)
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ ., data=Hitters[folds != j, ], nvmax=p)
# loopeo p veces para ver cómo es cada uno de los modelos con
# i variables (i entre 1 y p)
for (i in 1:p) {
# uso la predict() que fabriqué arriba
pred <- predict(best.fit, Hitters[folds == j, ], id=i)
cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)
}
}
# media de cada columna:
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type="b")
plot(mean.cv.errors, type="b", pch=20)
reg.best <- regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(reg.best, 10)
library(glmnet)
library(Matrix)
library(glmnet)
#### Regularización ####
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
model.matrix(Salary ~ ., Hitters)
model.matrix(Salary ~ ., Hitters)[, -1]
seq(10, -2, length=100)
# RIDGE:
## alpha = 0 => ridge
grid <- 10^seq(10, -2, length=100)
# RIDGE:
## creo la grilla para el valor de penalización
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid) ## alpha = 0 => ridge
dim(Hitters)
#### Regularización ####
x <- model.matrix(Salary ~ ., Hitters)[, -1] # la traspone
y <- Hitters$Salary
dim(X)
dim(x)
dim(Hitters) # n = 322, p = 19.
sum(is.na(Hitters$Salary)) # 59 datos faltantes.
Hitters <- na.omit(Hitters)
dim(Hitters) # n = 263, p = 19.
sum(is.na(Hitters$Salary)) # 0 datos faltantes.
#### Regularización ####
x <- model.matrix(Salary ~ ., Hitters)[, -1] # la traspone
y <- Hitters$Salary
# RIDGE:
## creo la grilla para el valor de penalización
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid) ## alpha = 0 => ridge
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
print("Se cargaron todas las librerías:
MASS, ISLR2, carData, car, boot, leaps, glmnet
")
}
load_libraries()
dim(coef(ridg.mod))
dim(coef(ridge.mod))
## cuando el lambda es muy grande => espero coeficientes más pequeños.
ridge.mod$lambda[50] # grande
coef(ridge.mod)[, 50]
## cuando el lambda es más pequeño => espero coeficientes más grandes.
ridge.mod$lambda[60] # chico.
coef(ridge.mod)[, 60] # son más grandes los coeficientes.
## Ridge para lambda = 50:
predict(ridge.mod, s=50, type="coefficients")[1:20, ]
## Uso train y test para MSE
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
## predigo para lambda = 4 en el set de test
ridge.mod <- glmnet(x[train, ], y[train], alpha=0,
lambda=grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx=x[test, ])
## calculo el MSE de test:
mean((ridge.pred - y.test)^2)
## Comparo ahora un valor de lambda muuuuy grande con
## fittear un modelo solo con Beta_0 (es decir, es solo mean(y[train]))
mean((mean(y[train]) - y.test)^2)
# con lambda muy grande:
ridge.pred <- predict(ridge.mod, s=1e10, newx=x[test, ])
mean((ridge.pred - y.test)^2)
## Y también, lambda = 4 es mucho mejor que lambda muy grande.
##
## Ahora pruebo si es mejor Ridge con lambda = 4 o mínimos cuadrados (lambda=0):
ridge.pred <- predict(ridge.mod, s=0, newx=x[test, ])
mean((ridge.pred - y.test)^2)
ridge.mod <- glmnet(x[train, ], y[train], alpha=0,
lambda=grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx=x[test, ])
## calculo el MSE de test:
mean((ridge.pred - y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod, s=0, exact=TRUE, type="coefficients", x=x[train, ], y=y[train])[1:20, ]
## Busco el mejor lambda con cv:
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv.out)
best_lambda <- cv.out$lambda.min
best_lambda
# MSE asociado con el menor lambda
ridge.pred <- predict(ridge.mod, s=best_lambda, newx=x[test, ])
mean((ridge.pred - y.test)^2)
## Ahora fitteo sobre todo el set de datos usando lambda=best_lambda
out <- glmnet(x, y, alpha=0)
predict(out, type="coefficients", s=best_lambda)[1:20, ]
# LASSO:
## único cambio es poner alpha=1
lasso.mod <- glmnet(x[train, ], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
# LASSO:
## único cambio es poner alpha=1
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
# CV y busco mejor lambda y MSE:
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
best_lambda <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod. s=best_lambda, newx=x[test, ])
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x[test, ])
mean((lasso.pred - y.test)^2)
## Pero, ahora cuando fitteo el modelo con el best_lambda, voy a ver
## que algunos coeficientes son seteados a 0:
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = s = bestlam)[1:20, ]
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20, ]
lasso.coef <- predict(out, type = "coefficients", s = best_lambda)[1:20, ]
lasso.coef
library(tree)
print("Se cargaron todas las librerías:",
"MASS, ISLR2, carData, car, boot, leaps, glmnet")
print("Se cargaron todas las librerías:")
print("MASS, ISLR2, carData, car, boot, leaps, glmnet")
print("Se cargaron todas las librerías:" + "MASS, ISLR2, carData, car, boot, leaps, glmnet")
print("Se cargaron todas las librerías:" + "MASS, ISLR2, carData, car, boot, leaps, glmnet")
#### Árboles de decisión
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0)
# fitteo un árbol mucho más grande:
longtree.boston <- tree(medv ~ ., Boston, subset=train,
control=tree.control(nobs = length(train), mindev = 0))
summary(longtree.boston)
# prunning tree original
cv.boston <- cv.tree(tree.boston)
View(cv.boston)
plot(cv.boston$size, cv.boston$dev, type="b")
# si quiero hacer el prunning:
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston, pretty=0)
yhat <- predict(tree.boston, newdata=Boston[train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
plot(yhat, boston.test, pch=20)
abline(0,1)
abline(0, 1, col="red")
mean((yhat-boston.test)^2)
mean((yhat-boston.test)^2) # 153.5446
mean((yhat-boston.test)^2) # 153.5446
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
mean((yhat-boston.test)^2) # 153.5446
plot(yhat, boston.test, pch=20)
abline(0, 1, col="red") # la media de los ptos debería pegarse
library(randomForest)
#### Ensambles ####
#### Bagging
set.seed(1)
bag.boston <- randomForest(medv ~ ., data=Boston,
subset=train, mtry=12, importance=TRUE)
bag.boston
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
plot(yhat.bag, boston.test, pch=20, col="chocolate")
abline(0, 1)
mean((yhat.bag - boston.test)^2)
## Cambio la cantidad de árboles (B):
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2) # 23.41916
mean((yhat.bag - boston.test)^2) # 25.75055
#### Random-Forest
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2) #
importance(rf.boston)
varImpPlot(rf.boston)
#### Boosting
set.seed(1)
# distribucion="gaussian" para problemas de regresión.
# n.tress = B.
# interaction.depth = D.
boost.boston <- gbm(medv ~ ., data=Boston[train, ],
distribution="gaussian", n.trees=5000, interaction.depth=4)
library(gbm)
#### Boosting
set.seed(1)
# distribucion="gaussian" para problemas de regresión.
# n.tress = B.
# interaction.depth = D.
boost.boston <- gbm(medv ~ ., data=Boston[train, ],
distribution="gaussian", n.trees=5000, interaction.depth=4)
summary(boost.boston)
# Plot de cada variable por separada para ver su influencia:
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
# cambio el lambda:
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2) #
library(BART)
install.packages("BART")
library(BART)
library(BART)
#### BART
# Tengo que crear matrices de los predictores:
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
# Test error
yhat.bar <- barfit$yhat.test.mean
mean((ytest - yhat.bar)^2)
# veo cuántas veces cada variable aparece en la colección de árboles:
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord]
setwd("~/Documents/AE/tp2")
d1<-read.table("student-mat.csv",sep=";",header=TRUE)
d2<-read.table("student-por.csv",sep=";",header=TRUE)
d3<-merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
View(d1)
d1<-read.table("student-mat.csv",sep=",",header=TRUE)
d2<-read.table("student-por.csv",sep=",",header=TRUE)
View(d1)
d3<-merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
print(nrow(d3)) # 382 students
View(d1)
View(d2)
View(d3)
View(d1)
View(d2)
is.na(d1)
sum(is.na(d1))
View(d1)
is.na(d2)
sum(is.na(d2))
View(d1)
View(d3)
dim(d1)
dim(d2)
dim(d3)
View(d1)
View(d3)
d3[, "G3.x"]
View(d3[, c("G3.x", "G3.y")])
math <- read.table("student-mat.csv",sep=",",header=TRUE)
por <- read.table("student-por.csv",sep=",",header=TRUE)
common <- merge(d1, d2,
by=c("school","sex","age","address","famsize","Pstatus","Medu",
"Fedu","Mjob","Fjob","reason","nursery","internet")
)
dim(math)
dim(por)
dim(common)
## Veo si las notas de ambos cursos están correlacionadas:
notas_com <- common[, c("G3.x", "G3.y")]
cor(notas_com)
d1 + d2
## concateno ambos sets de datos y enchufo una columna para decir si es de mate
## o de portugués:
df <- rbind(math, por)
View(df)
dim(df)
## concateno ambos sets de datos y enchufo una columna para decir si es de mate
## o de portugués:
math["course_type"] <- cbin("M", nrow(math))
## concateno ambos sets de datos y enchufo una columna para decir si es de mate
## o de portugués:
math["course_type"] <- cbind("M", nrow(math))
View(math)
## concateno ambos sets de datos y enchufo una columna para decir si es de mate
## o de portugués:
math["course_type"] <- rbind("M", nrow(math))
## concateno ambos sets de datos y enchufo una columna para decir si es de mate
## o de portugués:
math["course_type"] <- rep("M", nrow(math))
View(math)
por["course_type"] <- rep("P", nrow(por))
df <- rbind(math, por)
dim(df)
head(df)
dim(df)
n <- nrow(df)
p <- ncol(df)-1
setwd("~/Documents/AE/tp2")
write.csv(df, "~/Documents/AE/tp2", row.names=TRUE)
write.csv(df, "~/Documents/AE/tp2/df.csv", row.names=TRUE)
df <- read.table("df.csv",sep=",",header=TRUE)
n <- nrow(df)     # 1044
p <- ncol(df)-1   # 33
View(df)
n <- nrow(train)
# Lectura de datos ----
## Me quedo con portugués como train y matemática como test.
train <- read.table("student-por.csv",sep=",",header=TRUE)
test <- read.table("student-mat.csv",sep=",",header=TRUE)
p <- ncol(train)-1
n <- nrow(train)
dim(train)
dim(test)
df1 <- read.table("student-por.csv",sep=",",header=TRUE)
df2 <- read.table("student-mat.csv",sep=",",header=TRUE)
df <- rbind(df1, df2)
dim(df)
# train y test:
set.seed(9)
train <- sample(1:n, n*0.8)
test <- (-train)
View(df)
# train y test:
x <- model.matrix(G3 ~ ., df)[, -1]
y <- df$G3
x.train <- x[train, ]
x.test <- x[test, ]
y.train <- y[train]
y.test <- y[test]
dim(x.train)
dim(x.test)
n
n <- nrow(df)
p <- ncol(df)-1
# train y test:
x <- model.matrix(G3 ~ ., df)[, -1]
y <- df$G3
set.seed(9)
train <- sample(1:n, n*0.8)
test <- (-train)
x.train <- x[train, ]
x.test <- x[test, ]
y.train <- y[train]
y.test <- y[test]
dim(x.train)
dim(x.test)
dim(y.train)
nrow(y.train)
y.train
len(y.train)
length(y.train)
length(y.test)
