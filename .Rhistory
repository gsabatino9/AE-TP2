"varImpPlot(rf.fit)
imps <- importance(bag.fit)
ord <- order(imps[, 2], decreasing=T)
importancias <- imps[ord, 2]
total <- sum(importancias)
(importancias / total) * 100 # para ver porcentajes relativos."
# Dejo crecer más el árbol todavía
set.seed(9)
bag.fit2 <- randomForest(G3 ~ ., data=train,
mtry=p, ntree=1000)
y.hat.bag2 <- predict(bag.fit2, test)
calcular_ecm(y.hat.bag2) # lo mejora.
## Random Forest ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
comparaciones <- agregar_modelo("Random Forest", y.hat.rf)
#varImpPlot(rf.fit)
"
G2, G1, absences, failures, Mjob, Fedu, age, Medu, Fjob, health,
famrel.
"
## Boosting ----
boost.fit <- gbm(G3 ~ ., data=train,
distribution="gaussian", n.trees=5000)
summary(boost.fit)
y.hat.boost <- predict(boost.fit, test)
comparaciones <- agregar_modelo("Boosting", y.hat.boost)
## BART ----
xtrain <- train[, -c(33)]
ytrain <- train$G3
xtest <- test[, -c(33)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- rbind(comparaciones, data.frame(Modelo="BART", ECM.test=ECM_bart))
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord] # guardian como novedad
cmp.M <- as.data.frame(comparaciones)
names(cmp.M) <- c("Modelo", "ECM")
rbind(cmp.M$Modelo, cmp.M$ECM, cmp.P$ECM)
a <- rbind(cmp.M$Modelo, cmp.M$ECM, cmp.P$ECM)
a
View(a)
cbind(cmp.M$Modelo, cmp.M$ECM, cmp.P$ECM)
a <- data.frame(Modelo=cmp.M$Modelo, "ECM mat"=cmp.M$ECM, "ECM por"=cmp.P$ECM)
a
xtable(a, type="latex", digits=4)
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(jtools)
library(broom.mixed)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("mat_transf.csv",sep=",",header=TRUE)
por <- read.table("por_transf.csv",sep=",",header=TRUE)
mat <- mat[, -1]
por <- por[, -1]
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
mat <- mat[, -c(31, 32)]
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train <- mat[aux_train, ]
test <- mat[aux_test, ]
n <- nrow(train)
p <- ncol(train)-3
# Comparación de modelos ----
library(randomForestExplainer)
## Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 8.1731
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre, pred) {
ECM <- calcular_ecm(pred)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
## Árbol crecido ----
longtree.fit <- tree(G3 ~ ., train,
control=tree.control(nobs = nrow(train), mindev = 0))
summary(longtree.fit)
y.hat.tree <- predict(longtree.fit, test)
comparaciones <- agregar_modelo("Árbol crecido", y.hat.tree)
## Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
y.hat.prunning <- predict(prunning.fit, test)
comparaciones <- agregar_modelo("Árbol podado", y.hat.prunning)
## Bagging ----
set.seed(9)
bag.fit <- randomForest(G3 ~ ., data=train, mtry=p, importance=TRUE)
y.hat.bag <- predict(bag.fit, test)
comparaciones <- agregar_modelo("Bagging", y.hat.bag)
"varImpPlot(rf.fit)
imps <- importance(bag.fit)
ord <- order(imps[, 2], decreasing=T)
importancias <- imps[ord, 2]
total <- sum(importancias)
(importancias / total) * 100 # para ver porcentajes relativos."
# Dejo crecer más el árbol todavía
set.seed(9)
bag.fit2 <- randomForest(G3 ~ ., data=train,
mtry=p, ntree=1000)
y.hat.bag2 <- predict(bag.fit2, test)
calcular_ecm(y.hat.bag2) # lo mejora.
## Random Forest ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
comparaciones <- agregar_modelo("Random Forest", y.hat.rf)
#varImpPlot(rf.fit)
"
G2, G1, absences, failures, Mjob, Fedu, age, Medu, Fjob, health,
famrel.
"
## Boosting ----
boost.fit <- gbm(G3 ~ ., data=train,
distribution="gaussian", n.trees=5000)
summary(boost.fit)
y.hat.boost <- predict(boost.fit, test)
comparaciones <- agregar_modelo("Boosting", y.hat.boost)
## BART ----
xtrain <- train[, -c(31)]
ytrain <- train$G3
xtest <- test[, -c(31)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- rbind(comparaciones, data.frame(Modelo="BART", ECM.test=ECM_bart))
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord] # guardian como novedad
cmp.M <- as.data.frame(comparaciones)
names(cmp.M) <- c("Modelo", "ECM")
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(jtools)
library(broom.mixed)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("mat_transf.csv",sep=",",header=TRUE)
por <- read.table("por_transf.csv",sep=",",header=TRUE)
mat <- mat[, -1]
por <- por[, -1]
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por <- por[, -c(31, 32)]
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.8)
aux_test <- (-aux_train)
train <- por[aux_train, ]
test <- por[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
# Comparación de modelos ----
library(randomForestExplainer)
## Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 8.1731
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre, pred) {
ECM <- calcular_ecm(pred)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
## Árbol crecido ----
longtree.fit <- tree(G3 ~ ., train,
control=tree.control(nobs = nrow(train), mindev = 0))
summary(longtree.fit)
y.hat.tree <- predict(longtree.fit, test)
comparaciones <- agregar_modelo("Árbol crecido", y.hat.tree)
## Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
y.hat.prunning <- predict(prunning.fit, test)
comparaciones <- agregar_modelo("Árbol podado", y.hat.prunning)
## Bagging ----
set.seed(9)
bag.fit <- randomForest(G3 ~ ., data=train, mtry=p, importance=TRUE)
y.hat.bag <- predict(bag.fit, test)
comparaciones <- agregar_modelo("Bagging", y.hat.bag)
# Dejo crecer más el árbol todavía
set.seed(9)
bag.fit2 <- randomForest(G3 ~ ., data=train,
mtry=p, ntree=1000)
y.hat.bag2 <- predict(bag.fit2, test)
calcular_ecm(y.hat.bag2) # lo mejora.
## Random Forest ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
comparaciones <- agregar_modelo("Random Forest", y.hat.rf)
varImpPlot(rf.fit)
"
G2, G1, absences, failures, Mjob, Fedu, age, Medu, Fjob, health,
famrel.
"
## Boosting ----
boost.fit <- gbm(G3 ~ ., data=train,
distribution="gaussian", n.trees=5000)
summary(boost.fit)
y.hat.boost <- predict(boost.fit, test)
comparaciones <- agregar_modelo("Boosting", y.hat.boost)
## BART ----
xtrain <- train[, -c(31)]
ytrain <- train$G3
xtest <- test[, -c(31)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- rbind(comparaciones, data.frame(Modelo="BART", ECM.test=ECM_bart))
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord] # guardian como novedad
cmp.P <- as.data.frame(comparaciones)
names(cmp.P) <- c("Modelo", "ECM")
a <- data.frame(Modelo=cmp.M$Modelo, "ECM mat"=cmp.M$ECM, "ECM por"=cmp.P$ECM)
a
xtable(a, type="latex", digits=4)
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(jtools)
library(broom.mixed)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("mat_transf.csv",sep=",",header=TRUE)
por <- read.table("por_transf.csv",sep=",",header=TRUE)
mat <- mat[, -1]
por <- por[, -1]
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
mat <- mat[, -c(31, 32)]
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train <- mat[aux_train, ]
test <- mat[aux_test, ]
n <- nrow(train)
p <- ncol(train)-3
# Comparación de modelos ----
library(randomForestExplainer)
## Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 8.1731
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre, pred) {
ECM <- calcular_ecm(pred)
return(
rbind(comparaciones,
data.frame(Modelo=nombre, "ECM test"=ECM))
)
}
## Árbol crecido ----
longtree.fit <- tree(G3 ~ ., train,
control=tree.control(nobs = nrow(train), mindev = 0))
summary(longtree.fit)
y.hat.tree <- predict(longtree.fit, test)
comparaciones <- agregar_modelo("Árbol crecido", y.hat.tree)
## Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
y.hat.prunning <- predict(prunning.fit, test)
comparaciones <- agregar_modelo("Árbol podado", y.hat.prunning)
## Bagging ----
set.seed(9)
bag.fit <- randomForest(G3 ~ ., data=train, mtry=p, importance=TRUE)
y.hat.bag <- predict(bag.fit, test)
comparaciones <- agregar_modelo("Bagging", y.hat.bag)
"varImpPlot(rf.fit)
imps <- importance(bag.fit)
ord <- order(imps[, 2], decreasing=T)
importancias <- imps[ord, 2]
total <- sum(importancias)
(importancias / total) * 100 # para ver porcentajes relativos."
# Dejo crecer más el árbol todavía
set.seed(9)
bag.fit2 <- randomForest(G3 ~ ., data=train,
mtry=p, ntree=1000)
y.hat.bag2 <- predict(bag.fit2, test)
calcular_ecm(y.hat.bag2) # lo mejora.
## Random Forest ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
comparaciones <- agregar_modelo("Random Forest", y.hat.rf)
#varImpPlot(rf.fit)
"
G2, G1, absences, failures, Mjob, Fedu, age, Medu, Fjob, health,
famrel.
"
## Boosting ----
boost.fit <- gbm(G3 ~ ., data=train,
distribution="gaussian", n.trees=5000)
summary(boost.fit)
y.hat.boost <- predict(boost.fit, test)
comparaciones <- agregar_modelo("Boosting", y.hat.boost)
## BART ----
xtrain <- train[, -c(31)]
ytrain <- train$G3
xtest <- test[, -c(31)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- rbind(comparaciones, data.frame(Modelo="BART", ECM.test=ECM_bart))
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord] # guardian como novedad
# Gráficos ----
forest <- rf.fit
## Gráficos 1 ----
min_depth_frame <- min_depth_distribution(forest)
## Interacción ----
(vars <- important_variables(importance_frame, k = 5, measures = c("mean_min_depth", "no_of_trees")))
## Importancia de variables ----
importance_frame <- measure_importance(forest)
## Interacción ----
(vars <- important_variables(importance_frame, k = 5, measures = c("mean_min_depth", "no_of_trees")))
interactions_frame <- min_depth_interactions(forest, vars)
plot_min_depth_interactions(interactions_frame)
aux <- data.frame(Features=())
aux <- data.frame(Features=c())
names(mat)
names(mat)[-33]
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.8)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
# Comparación de modelos ----
ecm <- function(modelo, a_testear) {
y.hat <- predict(modelo, a_testear)
return(
c(mean((y.hat - a_testear$G3)^2), summary(modelo)$r.squared,
sum(summary(modelo)$coefficients[,4] < 0.01)-1
)
)
}
ecm_pred <- function(predicciones, a_testear) {
return(mean((predicciones - a_testear$G3)^2))
}
cmp.M <- matrix(ncol=3)
cmp.P <- matrix(ncol=3)
names(mat)[-33]
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(jtools)
library(broom.mixed)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("mat_transf.csv",sep=",",header=TRUE)
por <- read.table("por_transf.csv",sep=",",header=TRUE)
mat <- mat[, -1]
por <- por[, -1]
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
names(mat)[-33]
aux$Features <- names(mat)[-33]
aux <- data.frame()
aux$Features <- names(mat)[-33]
aux <- data.frame(Features=names(mat)[-33])
rep(1:32, "")
rep("", 1:32)
rep(NA, 1:32)
rep(NA, 32)
aux$M1 <- rep("", 32)
aux$M2 <- rep("", 32)
xtable(aux, type="latex", digits=4)
aux <- data.frame(Features=names(mat)[-33])
aux$M1 <- rep("", 32)
aux$M2 <- rep("", 32)
aux$M3 <- rep("", 32)
aux$M4 <- rep("", 32)
xtable(aux, type="latex", digits=4)
aux <- data.frame(rep("", 32))
row.names(aux) <- Features=names(mat)[-33]
aux <- data.frame(rep("", 32))
row.names(aux) <- names(mat)[-33]
aux$M2 <- rep("", 32)
aux$M3 <- rep("", 32)
aux$M4 <- rep("", 32)
aux
aux <- data.frame(M1=rep("", 32))
row.names(aux) <- names(mat)[-33]
aux$M2 <- rep("", 32)
aux$M3 <- rep("", 32)
aux$M4 <- rep("", 32)
aux
xtable(aux, type="latex", digits=4)
