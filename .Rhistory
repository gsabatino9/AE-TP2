#### BART ----
xtrain <- train[, 1:p]
ytrain <- train[, "G3"]
xtest <- test[, 1:p]
ytest <- test[, "G3"]
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
mean((ytest - yhat.bar)^2)
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- agregar_modelo("BART", ECM_bart)
comparaciones
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord
barfit$varcount.mean[ord]
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord]
## Me quedo con portugués como train y matemática como test.
train <- read.table("student-por.csv",sep=",",header=TRUE)
test <- read.table("student-mat.csv",sep=",",header=TRUE)
train[-c(3,13,14,15,30,31,32,33)] <-
lapply(train[-c(3,13,14,15,30,31,32,33)], factor)
test[-c(3,13,14,15,30,31,32,33)] <-
lapply(test[-c(3,13,14,15,30,31,32,33)], factor)
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
por["course_type"] = rep("P", nrow(por))
mat["course_type"] = rep("M", nrow(mat))
por["course_type"] = as.factor(por["course_type"])
por["course_type"]
por["course_type"] = rep("P", nrow(por))
por["course_type"]
mat["course_type"]
as.factor(por["course_type"])
lapply(por["course_type"], factor)
rbind(por, mat)
df <- rbind(por, mat)
str(df)
df["course_type"] = lapply(df["course_type"], factor)
str(df)
set.seed(9)
train <- sample(1:nrow(df), nrow(df)*0.7)
test <- -(train)
test <- (-train)
test <- !(train)
set.seed(9)
train <- sample(1:nrow(x), nrow(x)*0.7)
set.seed(9)
train <- sample(1:nrow(df), nrow(df)*0.7)
test <- (-train)
set.seed(9)
aux_train <- sample(1:nrow(df), nrow(df)*0.7)
aux_test <- (-aux_train)
train <- df[aux_train, ]
test <- df[aux_test, ]
nrow(train)
nrow(test)
n <- nrow(train)
p <- ncol(train)-1
str(train)
# uno Dalc y Walc a ver qué onda
train$Dalc
train$Walc
### Modelo nulo ----
y.hat <- mean(train$G3)
calcular_ecm <- function(y.hat) {
return(mean((y.hat - test$G3)^2))
}
ECM_modelo_nulo <- calcular_ecm(y.hat) # 23.15902
comparaciones <- data.frame(Modelo="Nulo", "ECM test"=ECM_modelo_nulo)
agregar_modelo <- function(nombre_modelo, resultado) {
return(
rbind(comparaciones, data.frame(Modelo=nombre_modelo, "ECM test"=resultado))
)
}
comparaciones
### Regresión lineal con todos los predictores ----
lm.fit <- lm(G3 ~ ., data=train)
summary(lm.fit)
y.hat <- predict(lm.fit, test)
ECM_reg_lineal1 <- calcular_ecm(y.hat)
comparaciones <- agregar_modelo("Reg. lineal 1", ECM_reg_lineal1)
comparaciones
## Análisis del modelo lineal:
### Análisis de supuestos
vif(lm.fit) # no presenta colinealidad.
lm.fit2 <- update(lm.fit, ~ . - G1 - G2)
summary(lm.fit2) # el test es significativo, por lo tanto
# hay otras variables significativas en este modelo que solo G1 y G2.
calcular_ecm(predict(lm.fit2, test)) # da gigante: 21.08093
### Selección de modelos ----
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
#### Forward-Selection:
regfit.fwd <- regsubsets(G3 ~ ., data=train, nvmax=p, method="forward")
# Me quedo con el mejor a partir de cv con K=10:
k <- 10
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="forward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.fwd <- apply(cv.errors, 2, mean)
mean.cv.errors.fwd
par(mfrow = c(1, 1))
plot(mean.cv.errors.fwd, type = "b", pch=20)
coef_fwd <- which.min(mean.cv.errors.fwd) # 2
coef(regfit.fwd, coef_fwd) # se queda solo con G1 y G2.
fwd.fit <- lm(G3 ~ G1 + G2, data=train)
y.hat <- predict(fwd.fit, test)
ECM_rl_fwd <- calcular_ecm(y.hat)
comparaciones <- agregar_modelo("FWD selection", ECM_rl_fwd)
comparaciones
#### Backward-Selection:
regfit.bwd <- regsubsets(G3 ~ ., data=train, nvmax=p, method="backward")
summary(regfit.bwd)
# Me quedo con el mejor a partir de cv con K=10:
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="backward")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.bwd <- apply(cv.errors, 2, mean)
mean.cv.errors.bwd
par(mfrow = c(1, 1))
plot(mean.cv.errors.bwd, type = "b", pch=20)
coef_bwd <- which.min(mean.cv.errors.bwd) # 2
coef(regfit.bwd, coef_bwd) # se queda solo con G1 y G2.
ECM_rl_bwd <- ECM_rl_fwd
comparaciones <- agregar_modelo("BWD selection", ECM_rl_bwd)
#### Mixed-Selection:
regfit.mix <- regsubsets(G3 ~ ., data=train, nvmax=p, method="seqrep")
summary(regfit.mix)
# Me quedo con el mejor a partir de cv con K=10:
set.seed(9)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, p,
dimnames = list(NULL, paste(1:p)))
for (j in 1:k) {
best.fit <- regsubsets(G3 ~ ., data=train[folds != j, ],
nvmax=p, method="seqrep")
for (i in 1:p) {
pred <- predict(best.fit, train[folds == j, ], id=i)
cv.errors[j, i] <- mean((train$G3[folds == j] - pred)^2)
}
}
mean.cv.errors.mix <- apply(cv.errors, 2, mean)
par(mfrow = c(1, 1))
plot(mean.cv.errors.mix, type = "b", pch=20)
coef_mix <- which.min(mean.cv.errors.mix) # 2
coef(regfit.mix, coef_mix) # se queda solo con G1 y G2.
coef(regfit.mix, 2)
coef(regfit.fwd, 2)
ECM_rl_mix <- ECM_rl_fwd
comparaciones <- agregar_modelo("MIXED selection", ECM_rl_mix)
comparaciones
### Mixed-Selection por AIC
set.seed(9)
step.model <- stepAIC(lm.fit, direction="both", trace=FALSE)
summary(step.model)
y.hat <- predict(step.model, test)
ECM_mix_aic <- calcular_ecm(y.hat)
comparaciones <- agregar_modelo("MIXED selection AIC", ECM_mix_aic)
comparaciones
### RIDGE ----
x.train <- model.matrix(G3 ~ ., train)[, -1]
y.train <- train$G3
x.test <- model.matrix(G3 ~ ., test)[, -1]
y.test <- test$G3
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x.train, y.train, alpha=0, lambda=grid, thresh=1e-12)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out)
best_lambda <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s=best_lambda, newx=x.test)
ECM_ridge <- calcular_ecm(ridge.pred)
comparaciones <- agregar_modelo("Ridge", ECM_ridge)
comparaciones
### LASSO ----
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
plot(lasso.mod)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
plot(cv.out)
best_lambda <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=best_lambda, newx=x.test)
ECM_lasso <-calcular_ecm(lasso.pred)
comparaciones <- agregar_modelo("Lasso", ECM_lasso)
comparaciones
### Árboles de decisión ----
#### Árbol crecido ----
longtree.fit <- tree(G3 ~ ., train,
control=tree.control(nobs = nrow(train), mindev = 0))
y.hat.tree <- predict(longtree.fit, test)
ECM_arbol_total <- calcular_ecm(y.hat.tree)
comparaciones <- agregar_modelo("Árbol crecido", ECM_arbol_total)
comparaciones
#### Árbol podado ----
prunning.fit <- prune.tree(longtree.fit, best=5)
plot(prunning.fit)
text(prunning.fit, pretty=0)
y.hat.prunning <- predict(prunning.fit, test)
ECM_prunning <- calcular_ecm(y.hat.prunning)
comparaciones <- agregar_modelo("Árbol podado", ECM_prunning)
comparaciones
### Ensambles ----
#### Bagging ----
set.seed(9)
bag.fit <- randomForest(G3 ~ ., data=train, mtry=p, importance=TRUE)
y.hat.bag <- predict(bag.fit, test)
ECM_bagging <- calcular_ecm(y.hat.bag)
comparaciones <- agregar_modelo("Bagging", ECM_bagging)
comparaciones
# Dejo crecer más el árbol todavía
set.seed(9)
bag.fit2 <- randomForest(G3 ~ ., data=train,
mtry=p, ntree=1000)
y.hat.bag2 <- predict(bag.fit2, test)
calcular_ecm(y.hat.bag2) # no lo mejora.
#### Random Forest ----
set.seed(9)
rf.fit <- randomForest(G3 ~ ., data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
ECM_rf <- calcular_ecm(y.hat.rf)
comparaciones <- agregar_modelo("Random Forest", ECM_rf)
comparaciones
#### Boosting ----
boost.fit <- gbm(G3 ~ ., data=train,
distribution="gaussian", n.trees=5000)
summary(boost.fit)
y.hat.boost <- predict(boost.fit, test)
ECM_boost <- calcular_ecm(y.hat.boost)
comparaciones <- agregar_modelo("Boosting", ECM_boost)
comparaciones
#### BART ----
xtrain <- train[, 1:p]
ytrain <- train[, "G3"]
xtest <- test[, 1:p]
ytest <- test[, "G3"]
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- agregar_modelo("BART", ECM_bart)
comparaciones
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord]
comparaciones
comparaciones %>% filter(!row_number() %in% 14)
comparaciones[-14,]
comparaciones <- comparaciones[-14,]
comparaciones
#### BART ----
xtrain <- train[, c(1:p)]
ytrain <- train$G3
#### BART ----
xtrain <- train[c(1:p)]
ytrain <- train$G3
xtest <- test[c(1:p)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ECM_bart <- mean((ytest - yhat.bar)^2)
ECM_bart
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord]
head(train[-"G3"])
head(train[-"G3", ])
head(train[p, ])
p
dim(train)
dim(train[p, ]=
dim(train[p, ])
dim(train[, p])
train[, p]
train[!(row.names(train) %in% c("G3")),]
head(train[!(row.names(train) %in% c("G3")),])
row.names(train)
col.names(train)
train[, "G3"]
train[, !"G3"]
train[, !c("G3")]
names(train)
p
train[, 33]
names(train[, 33])
train[, 34]
names(train[, -c(33)])
#### BART ----
xtrain <- train[, -c(33)]
ytrain <- train$G3
xtest <- test[, -c(33)]
ytest <- test$G3
set.seed(9)
barfit <- gbart(xtrain, ytrain, x.test=xtest)
yhat.bar <- barfit$yhat.test.mean
ord <- order(barfit$varcount.mean, decreasing=T)
barfit$varcount.mean[ord]
ECM_bart <- mean((ytest - yhat.bar)^2)
comparaciones <- agregar_modelo("BART", ECM_bart)
ECM_bart
comparaciones
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data, subset=index)
predict(boostrap.fit, test$G3[index])
}
boot(train, boot, R=1000)
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data, subset=index)
return(predict(boostrap.fit, test$G3[index]))
}
boot(train, boot, R=1000)
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data, subset=index)
predict(boostrap.fit, test$G3[index])
}
boot(train, boot.fn, R=1000)
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data, subset=index)
return(predict(boostrap.fit, test$G3[index]))
}
boot(train, boot.fn, R=1000)
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data, subset=index)
return(predict(bootstrap.fit, test$G3[index]))
}
boot(train, boot.fn, R=1000)
train
as.data.frame(train)
boot(as.data.frame(train), boot.fn, R=1000)
boot.fn(train, sample(nrow(train), nrow(train), replace=TRUE))
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data[index, ])
return(predict(bootstrap.fit, test$G3[index]))
}
boot.fn(train, sample(nrow(train), nrow(train), replace=TRUE))
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data[index, ])
return(predict(bootstrap.fit, test$G3[index]))
}
boot.fn(train, sample(nrow(train), nrow(train), replace=TRUE))
head(train)
type(train)
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data[index, ])
return(predict(bootstrap.fit, test[index, ]))
}
boot(train, boot.fn, R=1000)
set.seed(9)
bootstrap.fit <- boot(train, boot.fn, R=1000)
bootstrap.fit
head(boostrap.fit)
head(bootstrap.fit)
### Regresión lineal con muestras bootstrapeadas ----
boot.fn <- function(data, index) {
bootstrap.fit <- lm(G3 ~ G1+G2, data=data[index, ])
pred_aux <- predict(bootstrap.fit, test)
calcular_ecm(pred_aux)
}
set.seed(9)
bootstrap.fit <- boot(train, boot.fn, R=1000)
bootstrap.fit
comparaciones
coef_fwd
coef(regfit.fwd, coef_fwd) # se queda solo con G1 y G2.
### Regresión lineal agregando términos polinómicos al mejor modelo ----
# El mejor modelo es con G1+G2+course_type solamente
lm.fit3 <- lm(G3 ~ G1+G2+course_type, data=train)
y.hat.lm3 <- predict(lm.fit3, test)
calcular_ecm(y.hat.lm3)
comparaciones
calcular_ecm(y.hat.lm3)
lm.fit4 <- lm(G3 ~ G1*G2 + G1*course_type + G2*course_type, data=train)
y.hat.lm4 <- predict(lm.fit4, test)
calcular_ecm(y.hat.lm4)
summary(lm.fit4)
anova(lm.fit3, lm.fit4)
lm.fit4 <- lm(G3 ~ G1*G2 + course_type, data=train)
y.hat.lm4 <- predict(lm.fit4, test)
calcular_ecm(y.hat.lm4) # 2.154735
lm.fit4 <- lm(G3 ~ G1*G2 + G1*course_type + G2*course_type, data=train)
y.hat.lm4 <- predict(lm.fit4, test)
calcular_ecm(y.hat.lm4) # 2.154735
lm.fit5 <- lm(G3 ~ poly(G1, 3) + poly(G2, 3) + course_type, data=train)
y.hat.lm5 <- predict(lm.fit5, test)
calcular_ecm(y.hat.lm5)
summary(lm.fit5)
set.seed(9)
rf.fit <- randomForest(G3 ~ . - G1 - G2, data=train, mtry=p/3, importance=TRUE)
y.hat.rf <- predict(rf.fit, test)
calcular_ecm(y.hat.rf)
varImpPlot(rf.fit)
importance(rf.fit)
x.train <- model.matrix(G3 ~ ., train)[, -1]
y.train <- train$G3
x.test <- model.matrix(G3 ~ ., test)[, -1]
y.test <- test$G3
grid <- 10^seq(10, -2, length=100)
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=grid)
set.seed(9)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
plot(cv.out)
best_lambda <- cv.out$lambda.min
predict(lasso.mod, type="coefficients", s=best_lambda)
predict(lasso.mod, type="coefficients", s=best_lambda)[1:p, ]
library(lmtest)
head(train)
big.model <- lm(G3 ~ course_type + sex + age, data=train)
summary(big.model)
big.model <- lm(G3 ~ course_type + sex + age + reason, data=train)
summary(big.model)
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
por["course_type"] = rep("P", nrow(por))
mat["course_type"] = rep("M", nrow(mat))
df <- rbind(por, mat)
df["course_type"] = lapply(df["course_type"], factor)
set.seed(9)
aux_train <- sample(1:nrow(df), nrow(df)*0.7)
aux_test <- (-aux_train)
train <- df[aux_train, ]
test <- df[aux_test, ]
n <- nrow(train)
p <- ncol(train)-1
install.packages("drat")
install.packages("xgboost")
library(xgboost)
xg.fit <- xgboost(data=train, max.depth=2, eta=1, nthread = 2, nrounds = 2)
library("tidyverse")
library("caret")
install.packages("caret")
train <- map_df(train, function(columna) {
columna %>%
as.factor() %>%
as.numeric %>%
{ . - 1 }
})
head(train)
train_mat <- train %>%
select(-G3) %>%
as.matrix() %>%
xgb.DMatrix(data = ., label = train$G3)
train_matr
train_mat
test_mat <- test %>%
select(-G3) %>%
as.matrix() %>%
xgb.DMatrix(data = ., label = test$G3)
test <- map_df(test, function(columna) {
columna %>%
as.factor() %>%
as.numeric %>%
{ . - 1 }
})
test_mat <- test %>%
select(-G3) %>%
as.matrix() %>%
xgb.DMatrix(data = ., label = test$G3)
xg.fit <- xgboost(data = train_mat,
nrounds = 10, max.depth = 2, eta = 0.3, nthread = 2)
xg.fit
pred <- predict(xg.fit, test_mat)
calcular_ecm(pred)
pred
mean((pred-test$G3)^2)
install.packages("catboost")
library(catboost)
install.packages("adabag")
library(adabag)
ada.fit <- boosting(G3 ~ ., data=train)
ada.fit <- gbm(G3 ~ ., data=train,
distribution="adaboost", n.trees=5000, interaction.depth=4)
library(caret)
caret.xgb <- train(G3 ~ ., method = "ada", data = train,
trControl = trainControl(method = "cv", number = 5))
