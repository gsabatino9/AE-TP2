predict(lm.fit, data.frame(lstat = (c(5,10,15))), interval="prediction")
## plots
plot(lstat, medv)
## plots
plot(lstat, medv, pch=20)
## plots
plot(lstat, medv, pch=20, color="darkblue")
## plots
plot(lstat, medv, pch=20, col="darkblue")
abline(lm.fit)
abline(lm.fit, pch=5)
abline(lm.fit, pch=5, col="chocolate")
abline(lm.fit, lwd=3, col="chocolate")
## plots
plot(lstat, medv, pch="+", col="darkblue")
abline(lm.fit, lwd=3, col="chocolate")
#### Regresión lineal múltiple ####
lm.fit <- lm(medv ~ ., data=Boston)
load_libraries()
#### Regresión lineal simple ####
head(Boston)
#### Regresión lineal múltiple ####
lm.fit <- lm(medv ~ ., data=Boston)
summary(lm.fit)
## calculo el VIF
vif(lm.fit)
library(car)
library(carData)
library(car)
## calculo el VIF
vif(lm.fit)
## a la regresión que ya había utilizado, le saco una
## variable
lm.fit1 <- update(lm.fit, ~ . - age)
summary(lm.fit1)
## Interaction Terms
# Para incluir solo la interacción: X1:X2
# Para incluir la interacción junto con cada término: X1*X2
lm.inter <- lm(medv ~ lstat * age, data=Boston) # lstat, age y lstat*age
summary(lm.inter)
## Transformaciones no-lineales
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
## Transformaciones no-lineales
lm.fit2 <- lm(medv ~ lstat + I(lstat^2), data=Boston)
summary(lm.fit2)
## Computo la mejorar de lm.fit2 por sobre lm.fit1 con anova()
lm.fit <- lm(medv~lstat, data=Boston)
anova(lm.fit, lm.fit2)
plot(lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2, pch=20, col="chocolate")
par(mfrow=c(2,2))
plot(lm.fit2, pch=20, col="chocolate")
## Para crear un polinomio de grado 5, conteniendo
## desde X^1 hasta X^5 uso poly(X, 5):
lm.fit5 <- lm(medv ~ poly(lstat, 5))
## Para crear un polinomio de grado 5, conteniendo
## desde X^1 hasta X^5 uso poly(X, 5):
lm.fit5 <- lm(medv ~ poly(lstat, 5), data=Boston)
summary(lm.fit5)
## Predictores qualitativos (one-hot encoding)
head(Carseats)
# fitteo con algunas relaciones que propone el libro:
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data=Carseats)
summary(lm.fit)
attach(Carseats)
attach(Carseats)
# fitteo con algunas relaciones que propone el libro:
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data=Carseats)
summary(lm.fit)
constrasts(ShelveLoc)
contrasts(ShelveLoc)
load_libraries()
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
}
load_libraries()
#### Cross-validation y Bootstrap ####
##### Validation set
set.seed(1)
## divido
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset=train)
attach(Auto)
MSE.fit <- mean((mpg - predict(lm.fit, Auto))[-train^2])
mean((mpg - predict(lm.fit, Auto))[-train]^2)
mean((mpg - predict(lm.fit, Auto))[-train^2])
mean((mpg - predict(lm.fit, Auto))[-train]^2)
## calculo el MSE en lo que no es train
MSE.fit <- mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto, subset=train)
MSE.fit2 <- mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data=Auto, subset=train)
MSE.fit3 <- mean((mpg - predict(lm.fit3, Auto))[-train]^2)
## Pruebo con otro data-set
set.seed(2)
## Pruebo con otro data-set
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset=train)
MSE.fit <- mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto, subset=train)
MSE.fit2 <- mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data=Auto, subset=train)
MSE.fit3 <- mean((mpg - predict(lm.fit3, Auto))[-train]^2)
#### LOOCV
## para usarlo uso glm() que ya lo hace automáticamente
glm.fit <- glm(mpg ~ horsepower, data=Auto)
coef(glm.fit)
library(boot)
cv.err <- cv.glm(Auto, glm.fit)
View(cv.err)
# El MSE es:
cv.err$delta
cv.error <- rep(0,10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data=Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
which.min(cv.error)
plot(1:10, cv.error)
plot(1:10, cv.error)
plot(1:10, cv.error)
plot(1:10, cv.error, pch=20)
points(which.min(cv.error), col="red")
points(which.min(cv.error), col="red", pch=20)
plot(1:10, cv.error, pch=20)
points(which.min(cv.error), col="red", pch=19)
plot(1:10, cv.error, pch=20)
points(which.min(cv.error), cv.error[which.min(cv.error)], col="red", pch=19)
#### K-fold cv:
## puedo usar cv.glm poniendo K=lo que quiero
set.seed(17)
cv.error.10 <- rep(0,10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data=Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
points(which.min(cv.error.10), cv.error[which.min(cv.error.10)], col="red", pch=19)
plot(1:10, cv.error.10, pch=20)
points(which.min(cv.error.10), cv.error[which.min(cv.error.10)], col="red", pch=19)
# 1.
alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
alpha.fn(Portfolio, 1:100)
# una mini prueba del paso (2)
set.seed(7)
## Selecciono 100 observaciones (con repo) que van desde 1 a 100
alpha.fn(Portfolio, sample(100, 100, replace=TRUE))
# 2. con 1000 muestras bootstrapeadas
boot(Portfolio, alpha.fn, R=1000)
## Estimando Accuracy de una regresión lineal
boot.fn <- function(data, index)
## Estimando Accuracy de una regresión lineal
boot.fn <- function(data, index) {
coef(lm(mpg ~ horsepower, data=data, subset=index))
}
boot.fn(Auto, 1:392)
## Estimando Accuracy de una regresión lineal
boot.fn <- function(data, index) {
coef(lm(mpg ~ horsepower, data=data, subset=index))
}
boot.fn(Auto, 1:392)
# puedo hacer lo mismo que hice antes de usar muestras bootstrapeadas
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=TRUE))
boot.fn(Auto, sample(392, 392, replace=TRUE))
# puedo hacer lo mismo que hice antes de usar muestras bootstrapeadas
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=TRUE))
boot.fn(Auto, sample(392, 392, replace=TRUE))
# 2.
boot(Auto, boot.fn, R=1000)
## Me da prácticamente los coeficientes que me da summary(lm(...)):
summary(lm(mpg ~ horsepower, data=Auto))$coef
setwd("~/Documents/AE/tp2")
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("mat_transf.csv",sep=",",header=TRUE)
por <- read.table("por_transf.csv",sep=",",header=TRUE)
mat <- mat[, -1]
por <- por[, -1]
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.8)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
mat$G3 <- ifelse(mat$G3 == 0, 0, 1)
por$G3 <- ifelse(por$G3 == 0, 0, 1)
View(mat)
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.8)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
# Clasificación ----
glm.fits <- glm(G3 ~ ., data=train.M, family=binomial)
summary(glm.fits)
mat$G2 <- ifelse(mat$G2 == 0, 0, 1)
mat$G1 <- ifelse(mat$G1 == 0, 0, 1)
por$G2 <- ifelse(por$G2 == 0, 0, 1)
por$G1 <- ifelse(por$G1 == 0, 0, 1)
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.8)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
# Clasificación ----
glm.fits <- glm(G3 ~ ., data=train.M, family=binomial)
summary(glm.fits)
View(mat)
load_libraries <- function() {
library(MASS)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
mat$G3 <- ifelse(mat$G3 == 0, 0, 1)
por$G3 <- ifelse(por$G3 == 0, 0, 1)
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.8)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.8)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
# Clasificación ----
glm.fits <- glm(G3 ~ ., data=train.M, family=binomial)
summary(glm.fits)
predict(glm.fits, type="response")
glm.probs <- predict(glm.fits, type="response")
glm.probs[1:10]
sum(glm.probs < 1)
sum(glm.probs == 1)
sum(glm.probs < 0.5)
sum(glm.probs > 0.5)
sum(mat$G3 == 0)
table(glm.pred, mat$G3)
glm.pred <- rep(1, nrow(train.M))
glm.pred[glm.probs < 0.5] = 0
table(glm.pred, train.M$G3)
glm.probs <- predict(glm.fits, test.M, type="response")
glm.probs[1:10]
glm.pred <- rep(1, nrow(test.M))
glm.pred[glm.probs < 0.5] = 0
table(glm.pred, train.M$G3)
table(glm.pred, test.M$G3)
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.7)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
# Clasificación ----
glm.fits <- glm(G3 ~ ., data=train.M, family=binomial)
glm.probs <- predict(glm.fits, test.M, type="response")
glm.pred <- rep(1, nrow(test.M))
glm.pred[glm.probs < 0.5] = 0
table(glm.pred, test.M$G3)
mean(glm.pred == test.M)
103 / 106
8 / 13
summary(glm.fits)
# Clasificación 2 ----
library(class)
# Por
glm.fits <- glm(G3 ~ ., data=train.P, family=binomial)
summary(glm.fits)
glm.probs <- predict(glm.fits, test.M, type="response")
glm.probs[1:10]
glm.probs <- predict(glm.fits, test.P, type="response")
glm.probs[1:10]
glm.pred <- rep(1, nrow(test.P))
glm.pred[glm.probs < 0.5] = 0
table(glm.pred, test.P$G3)
mean(glm.pred == test.P)
2 / 8
set.seed(9)
train_X.M <- train.M[, -c(33)]
train_X.P <- train.P[, -c(33)]
test_X.M <- test.M[, -c(33)]
test_X.P <- test.P[, -c(33)]
train_Y.M <- train.M[, c(33)]
train_Y.P <- train.P[, c(33)]
test_Y.M <- test.M[, c(33)]
test_Y.P <- test.P[, c(33)]
knn.pred <- knn(train_X.M, test_X.M, tran_Y.M, k = 4)
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 4)
train_Y.M <- train.M$G3
train_Y.P <- train.P$G3
test_Y.M <- test.M$G3
test_Y.P <- test.P$G3
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 4)
table(knn.pred, test_Y.M)
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 1)
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat <- data.frame(lapply(mat, as.numeric))
por <- data.frame(lapply(por, as.numeric))
mat$G3 <- ifelse(mat$G3 == 0, 0, 1)
por$G3 <- ifelse(por$G3 == 0, 0, 1)
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.7)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
set.seed(9)
train_X.M <- train.M[, -c(33)]
train_X.P <- train.P[, -c(33)]
test_X.M <- test.M[, -c(33)]
test_X.P <- test.P[, -c(33)]
train_Y.M <- train.M$G3
train_Y.P <- train.P$G3
test_Y.M <- test.M$G3
test_Y.P <- test.P$G3
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 1)
is.na(mat)
sum(is.na(mat))
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(caret)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
por <- read.table("student-por.csv",sep=",",header=TRUE)
mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[-c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], factor)
mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(mat[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)] <-
lapply(por[c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33)], as.numeric)
mat$G3 <- ifelse(mat$G3 == 0, 0, 1)
por$G3 <- ifelse(por$G3 == 0, 0, 1)
mat <- data.frame(lapply(mat, as.numeric))
por <- data.frame(lapply(por, as.numeric))
sum(is.na(mat))
# Split train-test ----
set.seed(9)
aux_train <- sample(1:nrow(mat), nrow(mat)*0.7)
aux_test <- (-aux_train)
train.M <- mat[aux_train, ]
test.M <- mat[aux_test, ]
set.seed(9)
aux_train <- sample(1:nrow(por), nrow(por)*0.7)
aux_test <- (-aux_train)
train.P <- por[aux_train, ]
test.P <- por[aux_test, ]
set.seed(9)
train_X.M <- train.M[, -c(33)]
train_X.P <- train.P[, -c(33)]
test_X.M <- test.M[, -c(33)]
test_X.P <- test.P[, -c(33)]
train_Y.M <- train.M$G3
train_Y.P <- train.P$G3
test_Y.M <- test.M$G3
test_Y.P <- test.P$G3
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 1)
table(knn.pred, test_Y.M)
115 / 119
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 4)
table(knn.pred, test_Y.M)
# Mat
set.seed(9)
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 4)
table(knn.pred, test_Y.M)
# Por
set.seed(9)
knn.pred <- knn(train_X.P, test_X.P, train_Y.P, k = 4)
table(knn.pred, test_Y.P)
set.seed(9)
knn.pred <- knn(train_X.M, test_X.M, train_Y.M, k = 4)
table(knn.pred, test_Y.M)
