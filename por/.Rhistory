{ . - 1 }
})
as.numeric(mat)
as.numeric(mat$school)
as.numeric(mat$G3)
as.numeric(mat$G2)
aux <- mat
aux$G3
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
aux <- mat
aux$G3
lapply(mat, as.numeric)
head(lapply(mat, as.numeric))
mat2 <- lapply(mat, as.numeric)
table(mat$G3)
table(mat2$G3)
mat <- lapply(mat, as.numeric)
# Calculo binario: Aprobado / Desaprobado ----
mat$G3 <- ifelse(mat$G3 >= 12, 1 ,0)
clusters_bin <- data.frame(as.numeric(row.names(mat)))
## Clustering por fila ----
### Clustering jerárquico ----
hc.complete <- hclust(dist(mat), method="complete")
mat
data.frame(lapply(mat, as.numeric))
mat <- read.table("student-mat.csv",sep=",",header=TRUE)
mat[-c(3,13,14,15,30,31,32,33)] <-
lapply(mat[-c(3,13,14,15,30,31,32,33)], factor)
aux <- mat
mat <- data.frame(lapply(mat, as.numeric))
# Calculo binario: Aprobado / Desaprobado ----
mat$G3 <- ifelse(mat$G3 >= 12, 1 ,0)
clusters_bin <- data.frame(as.numeric(row.names(mat)))
## Clustering por fila ----
### Clustering jerárquico ----
hc.complete <- hclust(dist(mat), method="complete")
hc.average <- hclust(dist(mat), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
## Clustering por fila ----
### Clustering jerárquico ----
xsc <- scale(mat)
hc.complete <- hclust(dist(xsc), method="complete")
hc.average <- hclust(dist(xsc), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
abline(h=40, col="red")
abline(h=30, col="chocolate")
abline(h=20, col="blue")
abline(h=17, col="orange")
clusters_bin$JC1 <- cutree(hc.complete, h=40)
clusters_bin$JC2 <- cutree(hc.complete, h=30)
clusters_bin$JC3 <- cutree(hc.complete, h=20)
clusters_bin$JC4 <- cutree(hc.complete, h=17)
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
xsc <- scale(mat)
scaled.hc <- hclust(dist(xsc), method = "complete")
plot(scaled.hc,
main = "Hierarchical Clustering with Scaled Features")
abline(h=12, col="red")
abline(h=11.5, col="blue")
abline(h=10, col="chocolate")
plot(scaled.hc,
main = "Hierarchical Clustering with Scaled Features")
abline(h=11.5, col="blue")
abline(h=10, col="chocolate")
abline(h=11, col="red")
clusters_bin$JC5 <- cutree(scaled.hc, h=11)
clusters_bin$JC6 <- cutree(scaled.hc, h=11.5)
clusters_bin$JC7 <- cutree(scaled.hc, h=10)
### K-means ----
xsd <- scale(mat)
set.seed(9)
# pongo 4 para que tenga la misma cantidad que jerárquico
km.out <- kmeans(xsd, 4, nstart=20)
set.seed(9)
clusters_bin$K1 <- kmeans(xsd, 4, nstart=20)$cluster
set.seed(9)
clusters_bin$K2 <- kmeans(xsd, 5, nstart=20)$cluster
set.seed(9)
clusters_bin$K3 <- kmeans(xsd, 6, nstart=20)$cluster
set.seed(9)
clusters_bin$K4 <- kmeans(xsd, 7, nstart=20)$cluster
set.seed(9)
clusters_bin$K5 <- kmeans(xsd, 8, nstart=20)$cluster
### PCA + jerárquico ----
pr.out <- prcomp(mat, scale=TRUE)
# tendría que ver cuál explica más la varianza, no solo 5
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, main = "Complete Linkage + PCA",
xlab = "", sub = "", cex = .9) # mucho más balanceado.
abline(h=9, col="red")
abline(h=8, col="blue")
abline(h=7, col="chocolate")
abline(h=6, col="deepskyblue")
clusters_bin$JC8 <- cutree(hc.out, h=9)
clusters_bin$JC9 <- cutree(hc.out, h=8)
clusters_bin$JC10 <- cutree(hc.out, h=7)
clusters_bin$JC11 <- cutree(hc.out, h=6)
## Clustering por columna ----
mat <- t(mat)
### Jerárquico ----
hc.complete <- hclust(dist(mat), method="complete")
hc.average <- hclust(dist(mat), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9) # tremendo
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
### PCA + jerárquico ----
pr.out <- prcomp(mat, scale=TRUE)
# tendría que ver cuál explica más la varianza, no solo 5
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, main = "Complete Linkage + PCA",
xlab = "", sub = "", cex = .9)
# Calculo 4 niveles: MM, M, B, E ----
mat <- aux
mat <- data.frame(lapply(mat, as.numeric))
mat$G1 <- ifelse(mat$G1 > 16, 3,
ifelse(mat$G1 > 11, 2,
ifelse(mat$G1 > 6, 1, 0)))
mat$G2 <- ifelse(mat$G2 > 16, 3,
ifelse(mat$G2 > 11, 2,
ifelse(mat$G2 > 6, 1, 0)))
mat$G3 <- ifelse(mat$G3 > 16, 3,
ifelse(mat$G3 > 11, 2,
ifelse(mat$G3 > 6, 1, 0)))
table(mat$G3)
table(aux$G3)
clusters_4L <- data.frame(as.numeric(row.names(mat)))
## Clustering por fila ----
### Clustering jerárquico ----
hc.complete <- hclust(dist(mat), method="complete")
hc.average <- hclust(dist(mat), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
abline(h=20, col="red")
abline(h=15, col="chocolate")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
abline(h=20, col="red")
abline(h=12, col="chocolate")
clusters_4L$JC1 <- cutree(hc.complete, h=20)
clusters_4L$JC2 <- cutree(hc.complete, h=12)
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
xsc <- scale(mat)
scaled.hc <- hclust(dist(xsc), method = "complete")
plot(scaled.hc,
main = "Hierarchical Clustering with Scaled Features")
abline(h=12, col="blue")
abline(h=11, col="red")
abline(h=10, col="chocolate")
abline(h=8, col="deepskyblue")
clusters_bin$JC3 <- cutree(scaled.hc, h=12)
clusters_bin$JC4 <- cutree(scaled.hc, h=11)
clusters_bin$JC5 <- cutree(scaled.hc, h=10)
clusters_bin$JC6 <- cutree(scaled.hc, h=8)
### K-means ----
xsd <- scale(mat)
set.seed(9)
# pongo 4 para que tenga la misma cantidad que jerárquico
km.out <- kmeans(xsd, 4, nstart=20)
set.seed(9)
clusters_4L$K1 <- kmeans(xsd, 4, nstart=20)$cluster
set.seed(9)
clusters_4L$K2 <- kmeans(xsd, 5, nstart=20)$cluster
set.seed(9)
clusters_4L$K3 <- kmeans(xsd, 6, nstart=20)$cluster
set.seed(9)
clusters_4L$K4 <- kmeans(xsd, 7, nstart=20)$cluster
set.seed(9)
clusters_4L$K5 <- kmeans(xsd, 8, nstart=20)$cluster
### PCA + jerárquico ----
pr.out <- prcomp(mat, scale=TRUE)
# tendría que ver cuál explica más la varianza, no solo 5
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, main = "Complete Linkage + PCA",
xlab = "", sub = "", cex = .9) # mucho más balanceado.
abline(h=9, col="red")
abline(h=8, col="blue")
abline(h=7, col="chocolate")
abline(h=6, col="deepskyblue")
abline(h=5, col="deepskyblue")
names(clusters_4L)
clusters_4L$JC3 <- cutree(scaled.hc, h=12)
clusters_4L$JC4 <- cutree(scaled.hc, h=11)
clusters_4L$JC5 <- cutree(scaled.hc, h=10)
clusters_4L$JC6 <- cutree(scaled.hc, h=8)
clusters_4L$JC7 <- cutree(hc.out, h=9)
clusters_4L$JC8 <- cutree(hc.out, h=8)
clusters_4L$JC9 <- cutree(hc.out, h=7)
clusters_4L$JC10 <- cutree(hc.out, h=6)
clusters_4L$JC11 <- cutree(hc.out, h=5)
abline(h=5, col="orange")
## Clustering por columna ----
mat <- t(mat)
### Jerárquico ----
hc.complete <- hclust(dist(mat), method="complete")
hc.average <- hclust(dist(mat), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9) # tremendo
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
hc.scale <- hclust(dist(scale(mat)), method="complete")
plot(hc.scale, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
### PCA + jerárquico ----
pr.out <- prcomp(mat, scale=TRUE)
# tendría que ver cuál explica más la varianza, no solo 5
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, main = "Complete Linkage + PCA",
xlab = "", sub = "", cex = .9)
setwd("~/Documents/AE/tp2/por")
# PORTUGUÉS ----
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
library("tidyverse")
library("caret")
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
aux <- por
por <- data.frame(lapply(por, as.numeric))
# PORTUGUÉS ----
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
library("tidyverse")
library("caret")
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
library("tidyverse")
library("caret")
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
aux <- por
por <- data.frame(lapply(por, as.numeric))
por$G3 <- ifelse(por$G3 >= 12, 1 ,0)
por$G3 <- ifelse(por$G1 >= 12, 1 ,0)
por$G3 <- ifelse(por$G2 >= 12, 1 ,0)
clusters_bin <- data.frame(as.numeric(row.names(por)))
## Clustering por fila ----
### Clustering jerárquico ----
hc.complete <- hclust(dist(por), method="complete")
hc.average <- hclust(dist(por), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
abline(h=20, col="red")
abline(h=15, col="chocolate")
abline(h=13, col="blue")
clusters_bin$JC1 <- cutree(hc.complete, h=20)
clusters_bin$JC2 <- cutree(hc.complete, h=15)
clusters_bin$JC3 <- cutree(hc.complete, h=13)
xsc <- scale(por)
scaled.hc <- hclust(dist(xsc), method = "complete")
plot(scaled.hc,
main = "Hierarchical Clustering with Scaled Features")
abline(h=13, col="red")
abline(h=12, col="chocolate")
abline(h=10, col="blue")
abline(h=9, col="deepskyblue")
clusters_bin$JC4 <- cutree(scaled.hc, h=13)
clusters_bin$JC5 <- cutree(scaled.hc, h=12)
clusters_bin$JC6 <- cutree(scaled.hc, h=10)
clusters_bin$JC7 <- cutree(scaled.hc, h=9)
xsd <- scale(por)
set.seed(9)
# pongo 4 para que tenga la misma cantidad que jerárquico
km.out <- kmeans(xsd, 4, nstart=20)
set.seed(9)
clusters_bin$K1 <- kmeans(xsd, 4, nstart=20)$cluster
set.seed(9)
clusters_bin$K2 <- kmeans(xsd, 5, nstart=20)$cluster
set.seed(9)
clusters_bin$K3 <- kmeans(xsd, 6, nstart=20)$cluster
set.seed(9)
clusters_bin$K4 <- kmeans(xsd, 7, nstart=20)$cluster
set.seed(9)
clusters_bin$K5 <- kmeans(xsd, 8, nstart=20)$cluster
### PCA + jerárquico ----
pr.out <- prcomp(por, scale=TRUE)
# tendría que ver cuál explica más la varianza, no solo 5
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, main = "Complete Linkage + PCA",
xlab = "", sub = "", cex = .9) # mucho más balanceado.
abline(h=10, col="red")
abline(h=9, col="blue")
abline(h=8, col="chocolate")
abline(h=7, col="deepskyblue")
abline(h=6, col="green")
abline(h=4, col="pink")
abline(h=4, col="pink", lwd=2)
clusters_bin$JC8 <- cutree(hc.out, h=10)
clusters_bin$JC9 <- cutree(hc.out, h=9)
clusters_bin$JC10 <- cutree(hc.out, h=8)
clusters_bin$JC11 <- cutree(hc.out, h=7)
clusters_bin$JC12 <- cutree(hc.out, h=6)
clusters_bin$JC13 <- cutree(hc.out, h=4)
## Clustering por columna ----
por <- t(por)
### Jerárquico ----
hc.complete <- hclust(dist(por), method="complete")
hc.average <- hclust(dist(por), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9) # tremendo
t(por)$G1
por
por[1,]
por[,1]
por$G2 <- ifelse(por$G1 >= 12, 1 ,0)
por$G1 <- ifelse(por$G2 >= 12, 1 ,0)
por <- aux
por <- data.frame(lapply(por, as.numeric))
## Clustering por columna ----
por <- t(por)
### Jerárquico ----
hc.complete <- hclust(dist(por), method="complete")
hc.average <- hclust(dist(por), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9) # tremendo
# Calculo binario: Aprobado / Desaprobado ----
por$G3 <- ifelse(por$G3 >= 12, 1 ,0)
por <- aux
# Calculo binario: Aprobado / Desaprobado ----
por$G3 <- ifelse(por$G3 >= 12, 1 ,0)
por$G1 <- ifelse(por$G1 >= 12, 1 ,0)
por$G2 <- ifelse(por$G2 >= 12, 1 ,0)
clusters_bin <- data.frame(as.numeric(row.names(por)))
## Clustering por columna ----
por <- t(por)
### Jerárquico ----
hc.complete <- hclust(dist(por), method="complete")
hc.average <- hclust(dist(por), method="average")
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
library("tidyverse")
library("caret")
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
por <- data.frame(lapply(por, as.numeric))
aux <- por
# Calculo binario: Aprobado / Desaprobado ----
por$G3 <- ifelse(por$G3 >= 12, 1 ,0)
por$G1 <- ifelse(por$G1 >= 12, 1 ,0)
por$G2 <- ifelse(por$G2 >= 12, 1 ,0)
clusters_bin <- data.frame(as.numeric(row.names(por)))
## Clustering por columna ----
por <- t(por)
### Jerárquico ----
hc.complete <- hclust(dist(por), method="complete")
hc.average <- hclust(dist(por), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9) # tremendo
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
### PCA + jerárquico ----
pr.out <- prcomp(por, scale=TRUE)
# tendría que ver cuál explica más la varianza, no solo 5
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, main = "Complete Linkage + PCA",
xlab = "", sub = "", cex = .9)
### Con distancia: Correlación ----
dd <- as.dist(1 - cor(t(por)))
hc.complete <- hclust(dd, method="complete")
hc.average <- hclust(dd, method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9) # tremendo
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
# Lectura de datos ----
## Importo librerías
load_libraries <- function() {
library(MASS)
library(ISLR2)
library(carData)
library(car)
library(boot) # para usar cv.glm() para cv y boot() para bootstrap
library(leaps) # para subset selection
library(Matrix)
library(glmnet) # para Ridge y Lasso
library(tree)
library(randomForest) # para bagging y randomForest
library(gbm) # para boosting
library(BART) # para Bayesian Additive Regression Trees
library(ggplot2)
library(dplyr)
library("tidyverse")
library("caret")
print("MASS, ISLR2, carData, car, boot, leaps, glmnet, tree, randomForest, gbm, BART")
}
load_libraries()
por <- read.table("student-por.csv",sep=",",header=TRUE)
por[-c(3,13,14,15,30,31,32,33)] <-
lapply(por[-c(3,13,14,15,30,31,32,33)], factor)
por <- data.frame(lapply(por, as.numeric))
aux <- por
# Calculo binario: Aprobado / Desaprobado ----
por$G3 <- ifelse(por$G3 >= 12, 1 ,0)
por$G1 <- ifelse(por$G1 >= 12, 1 ,0)
por$G2 <- ifelse(por$G2 >= 12, 1 ,0)
clusters_bin <- data.frame(as.numeric(row.names(por)))
## Clustering por fila ----
### Clustering jerárquico ----
hc.complete <- hclust(dist(por), method="complete")
hc.average <- hclust(dist(por), method="average")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
abline(h=20, col="red")
abline(h=15, col="chocolate")
abline(h=13, col="blue")
clusters_bin$JC1 <- cutree(hc.complete, h=20)
clusters_bin$JC2 <- cutree(hc.complete, h=15)
clusters_bin$JC3 <- cutree(hc.complete, h=13)
xsc <- scale(por)
scaled.hc <- hclust(dist(xsc), method = "complete")
plot(scaled.hc,
main = "Hierarchical Clustering with Scaled Features")
abline(h=13, col="red")
abline(h=12, col="chocolate")
abline(h=10, col="blue")
abline(h=9, col="deepskyblue")
clusters_bin$JC4 <- cutree(scaled.hc, h=13)
clusters_bin$JC5 <- cutree(scaled.hc, h=12)
clusters_bin$JC6 <- cutree(scaled.hc, h=10)
clusters_bin$JC7 <- cutree(scaled.hc, h=9)
dd <- as.dist(1-cor(t(por)))
hc.complete <- hclust(dd, method="complete")
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
abline(h=0.3, col="red")
abline(h=0..2, col="chocolate")
abline(h=0.2, col="chocolate")
abline(h=0.1, col="blue")
abline(h=0.15, col="blue")
clusters_bin$JCor1 <- cutree(hc.complete, h=0.3)
clusters_bin$JCor2 <- cutree(hc.complete, h=0.2)
clusters_bin$JCor3 <- cutree(hc.complete, h=0.15)
